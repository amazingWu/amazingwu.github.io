<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[关联规则-Apriori算法]]></title>
      <url>%2F2017%2F05%2F14%2F%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99-Apriori%E7%AE%97%E6%B3%95%2F</url>
      <content type="text"><![CDATA[关联规则的挖掘分为两步：1，找出所有频繁项集；2，由频繁项集产生强关联规则。总体的性能由第一步决定。Apriori核心算法思想简要描述如下：该算法中有两个关键步骤为连接步和剪枝步。1）连接步：为找出Lk(频繁k项集)，通过Lk-1与自身连接，产生候选k项集，该候选k项集，该候选集记作Ck；其中Lk-1的元素是可连接的。2）剪枝步：Ck是Lk的超集，即它的成员可以是也可以不是频繁的，但所有的频繁项集都包含在Ck中。扫描数据库，确定Ck中每一个候选的计数，从而确定Lk（计数值步小于最小支持度计数的所有候选是频繁的，从而属于Lk）。然而，Ck可能很大，这样涉及的计算量就很大。为了压缩Ck，使用Apriori性质：任何非频繁的（k-1）项集都不可能是频繁k项集的子集（推论出：频繁k项集的任何非空真子集都必需是频繁的）。因此，如果一个候选K项集（k-1）项集不在Lk中，则该候选项不可能是频繁的，从而可以由Ck中删除（可利用该定论来剪枝：只需检验（k-1）项集是不是频繁的就可以了，因为小于k-1项集已经在得到（k-1）项集时得到了验证）。 其中比较容易产生疑问的是自连接这里，过程可以用下图表示： 算法步骤： 扫描全部数据，产生候选1-项集的集合C1; 根据最小支持度，由候选1-项集的集合C1产生频繁1-项集的集合L1； 对k&gt;1，重复执行步骤4、5、6 由Lk执行连接（自连接）和剪枝操作，产生候选（k+1）-项集的集合Ck+1； 根据最小支持度，由候选（k+1）-项集的集合Ck+1，产生频繁（k+1）-项集的集合Lk+1； 若L!=空集，则k=k+1，跳往步骤4；否则跳往步骤7； 根据最小置信度，由频繁项集产生强关联规则，结束。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习初涉-高斯混合模型聚类]]></title>
      <url>%2F2017%2F05%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%B6%89-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E8%81%9A%E7%B1%BB%2F</url>
      <content type="text"><![CDATA[转载自： http://blog.pluskid.org/?p=39 漫谈 Clustering (3): Gaussian Mixture Model 实上，GMM 和 k-means 很像，不过 GMM 是学习出一些概率密度函数来（所以 GMM 除了用在 clustering 上之外，还经常被用于 density estimation ），简单地说，k-means 的结果是每个数据点被 assign 到其中某一个 cluster 了，而 GMM 则给出这些数据点被 assign 到每个 cluster 的概率，又称作 soft assignment 。得出一个概率有很多好处，因为它的信息量比简单的一个结果要多，比如，我可以把这个概率转换为一个 score ，表示算法对自己得出的这个结果的把握。也许我可以对同一个任务，用多个方法得到结果，最后选取“把握”最大的那个结果；另一个很常见的方法是在诸如疾病诊断之类的场所，机器对于那些很容易分辨的情况（患病或者不患病的概率很高）可以自动区分，而对于那种很难分辨的情况，比如，49% 的概率患病，51% 的概率正常，如果仅仅简单地使用 50% 的阈值将患者诊断为“正常”的话，风险是非常大的，因此，在机器对自己的结果把握很小的情况下，会“拒绝发表评论”，而把这个任务留给有经验的医生去解决。 我们已经使用过k-means算法解决聚类问题。这个算法的突出优点是简单易用，计算量也不多。然而，往往过于简单也是一个缺点。假设聚类可以表示为单个点往往会过于粗糙。举一个例子，如下图所示： 这个例子中数据位于同心圆。在这种情况下，标准的K均值由于两个圆的均值位置相同，无法把数据划分成簇（所以上面有一个绿点不知道该往哪跑，因为它没有簇）。因此，以距离模型为聚类标准的方法不一定都能成功适用。为了解决这些缺点，我们介绍一种用统计混合模型进行聚类的方法——高斯混合模型（Gaussian Mixture Model, GMM）。这种聚类方法得到的是每个样本点属于各个类的概率，而不是判定它完全属于一个类，所以有时也会被称为软聚类。 废话说了一堆，不过，在回到 GMM 之前，我们再稍微扯几句。我们知道，不管是机器还是人，学习的过程都可以看作是一种“归纳”的过程，在归纳的时候你需要有一些假设的前提条件，例如，当你被告知水里游的那个家伙是鱼之后，你使用“在同样的地方生活的是同一种东西”这类似的假设，归纳出“在水里游的都是鱼”这样一个结论。当然这个过程是完全“本能”的，如果不仔细去想，你也不会了解自己是怎样“认识鱼”的。另一个值得注意的地方是这样的假设并不总是完全正确的，甚至可以说总是会有这样那样的缺陷的，因此你有可能会把虾、龟、甚至是潜水员当做鱼。也许你觉得可以通过修改前提假设来解决这个问题，例如，基于“生活在同样的地方并且穿着同样衣服的是同一种东西”这个假设，你得出结论：在水里有并且身上长有鳞片的是鱼。可是这样还是有问题，因为有些没有长鳞片的鱼现在又被你排除在外了。 在这个问题上，机器学习面临着和人一样的问题，在机器学习中，一个学习算法也会有一个前提假设，这里被称作“归纳偏执 (bias)”（bias 这个英文词在机器学习和统计里还有其他许多的意思）。例如线性回归，目的是要找一个函数尽可能好地拟合给定的数据点，它的归纳偏执就是“满足要求的函数必须是线性函数”。一个没有归纳偏执的学习算法从某种意义上来说毫无用处，就像一个完全没有归纳能力的人一样，在第一次看到鱼的时候有人告诉他那是鱼，下次看到另一条鱼了，他并不知道那也是鱼，因为两条鱼总有一些地方不一样的，或者就算是同一条鱼，在河里不同的地方看到，或者只是看到的时间不一样，也会被他认为是不同的，因为他无法归纳，无法提取主要矛盾、忽略次要因素，只好要求所有的条件都完全一样──然而哲学家已经告诉过我们了：世界上不会有任何样东西是完全一样的，所以这个人即使是有无比强悍的记忆力，也绝学不到任何一点知识。 这个问题在机器学习中称作“过拟合 (Overfitting)”，例如前面的回归的问题，如果去掉“线性函数”这个归纳偏执，因为对于 N 个点，我们总是可以构造一个 N-1 次多项式函数，让它完美地穿过所有的这 N 个点，或者如果我用任何大于 N-1 次的多项式函数的话，我甚至可以构造出无穷多个满足条件的函数出来。如果假定特定领域里的问题所给定的数据个数总是有个上限的话，我可以取一个足够大的 N ，从而得到一个（或者无穷多个）“超级函数”，能够 fit 这个领域内所有的问题。然而这个（或者这无穷多个）“超级函数”有用吗？只要我们注意到学习的目的（通常）不是解释现有的事物，而是从中归纳出知识，并能应用到新的事物上，结果就显而易见了。 没有归纳偏执或者归纳偏执太宽泛会导致 Overfitting ，然而另一个极端──限制过大的归纳偏执也是有问题的：如果数据本身并不是线性的，强行用线性函数去做回归通常并不能得到好结果。难点正在于在这之间寻找一个平衡点。不过人在这里相对于（现在的）机器来说有一个很大的优势：人通常不会孤立地用某一个独立的系统和模型去处理问题，一个人每天都会从各个来源获取大量的信息，并且通过各种手段进行整合处理，归纳所得的所有知识最终得以统一地存储起来，并能有机地组合起来去解决特定的问题。这里的“有机”这个词很有意思，搞理论的人总能提出各种各样的模型，并且这些模型都有严格的理论基础保证能达到期望的目的，然而绝大多数模型都会有那么一些“参数”（例如 K-means 中的 k ），通常没有理论来说明参数取哪个值更好，而模型实际的效果却通常和参数是否取到最优值有很大的关系，我觉得，在这里“有机”不妨看作是所有模型的参数已经自动地取到了最优值。另外，虽然进展不大，但是人们也一直都期望在计算机领域也建立起一个统一的知识系统（例如语意网就是这样一个尝试）。 废话终于说完了，回到 GMM 。按照我们前面的讨论，作为一个流行的算法，GMM 肯定有它自己的一个相当体面的归纳偏执了。其实它的假设非常简单，顾名思义，Gaussian Mixture Model ，就是假设数据服从 Mixture Gaussian Distribution ，换句话说，数据可以看作是从数个 Gaussian Distribution 中生成出来的。实际上，我们在 K-means 和 K-medoids 两篇文章中用到的那个例子就是由三个 Gaussian 分布从随机选取出来的。实际上，从中心极限定理可以看出，Gaussian 分布（也叫做正态 (Normal) 分布）这个假设其实是比较合理的，除此之外，Gaussian 分布在计算上也有一些很好的性质，所以，虽然我们可以用不同的分布来随意地构造 XX Mixture Model ，但是还是 GMM 最为流行。另外，Mixture Model 本身其实也是可以变得任意复杂的，通过增加 Model 的个数，我们可以任意地逼近任何连续的概率密分布。 每个 GMM 由 K 个 Gaussian 分布组成，每个 Gaussian 称为一个“Component”，这些 Component 线性加成在一起就组成了 GMM 的概率密度函数：根据上面的式子，如果我们要从 GMM 的分布中随机地取一个点的话，实际上可以分为两步：首先随机地在这 K 个 Component 之中选一个，每个 Component 被选中的概率实际上就是它的系数 πk ,选中了 Component 之后，再单独地考虑从这个 Component 的分布中选取一个点就可以了──这里已经回到了普通的 Gaussian 分布，转化为了已知的问题。 那么如何用 GMM 来做 clustering 呢？其实很简单，现在我们有了数据，假定它们是由 GMM 生成出来的，那么我们只要根据数据推出 GMM 的概率分布来就可以了，然后 GMM 的 K 个 Component 实际上就对应了 K 个 cluster 了。根据数据来推算概率密度通常被称作 density estimation ，特别地，当我们在已知（或假定）了概率密度函数的形式，而要估计其中的参数的过程被称作“参数估计”。 现在假设我们有 N 个数据点，并假设它们服从某个分布（记作 p(x) ），现在要确定里面的一些参数的值，例如，在 GMM 中，我们就需要确定 \pi_k、\mu_k 和 \Sigma_k 这些参数。 我们的想法是，找到这样一组参数，它所确定的概率分布生成这些给定的数据点的概率最大，而这个概率实际上就等于我们把这个乘积称作似然函数 (Likelihood Function)。通常单个点的概率都很小，许多很小的数字相乘起来在计算机里很容易造成浮点数下溢，因此我们通常会对其取对数，把乘积变成加和得到 log-likelihood function 。接下来我们只要将这个函数最大化（通常的做法是求导并令导数等于零，然后解方程），亦即找到这样一组参数值，它让似然函数取得最大值，我们就认为这是最合适的参数，这样就完成了参数估计的过程。 下面让我们来看一看 GMM 的 log-likelihood function： 由于在对数函数里面又有加和，我们没法直接用求导解方程的办法直接求得最大值。为了解决这个问题，我们采取之前从 GMM 中随机选点的办法：分成两步，实际上也就类似于 K-means 的两步。 估计数据由每个 Component 生成的概率（并不是每个 Component 被选中的概率）：对于每个数据 Xi 来说，它由第 k 个 Component 生成的概率为由于式子里的 μk 和 Σk 也是需要我们估计的值，我们采用迭代法，在计算 γ(i, k) 的时候我们假定 μk 和 Σk 均已知，我们将取上一次迭代所得的值（或者初始值）。 估计每个 Component 的参数：现在我们假设上一步中得到的 γ(i, k) 就是正确的“数据 Xi 由 Component k 生成的概率”，亦可以当做该 Component 在生成这个数据上所做的贡献，或者说，我们可以看作Xi 这个值其中有 γ(i, k)Xi 这部分是由 Component k 所生成的。集中考虑所有的数据点，现在实际上可以看作 Component 生成了 γ(1, k)X1, … , γ(n, k)Xn 这些点。由于每个 Component 都是一个标准的 Gaussian 分布，可以很容易分布求出最大似然所对应的参数值：其中 ，并且 πk 也顺理成章地可以估计为 Nk/N 。 重复迭代前面两步，直到似然函数的值收敛为止。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习初涉--层次聚类]]></title>
      <url>%2F2017%2F05%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%B6%89-%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%2F</url>
      <content type="text"><![CDATA[&emsp;&emsp;层次聚类算法，是通过将数据组织为若干组并形成一个相应的树来进行聚类的。 层次聚类可以自底向上也可以自顶向下，自顶向下的叫分裂的聚类算法，自底向上的叫凝聚的聚类算法。&emsp;&emsp;一个简单的凝聚的层次聚类过程如图所示，一个完全层次聚类的质量由于无法对已经做的合并或分解进行调整而受到影响（简单的说就是一个点被划分到一个簇，或者两个簇合并后，后续无法进行改变，将会一直影响到聚类结束）。但是层次聚类算法没有使用准则函数，它所含的对数据结构的假设更少，所以它的通用性更强。&emsp;&emsp;凝聚的层次聚类：自底向上的策略，首先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇，直到所有的对象都在一个簇中或者达到某个终结条件。&emsp;&emsp;分裂的层次聚类：顶向下的策略，首先将所有的簇都放在一个簇中，然后慢慢地细分为越来越小的簇，直到每个对象自成一簇或者达到其他的一个终结条件，例如满足了某个期望的簇数目，又或者两个最近的簇之间的距离达到了某一个阈值。说明：在凝聚的层次聚类中，两个簇之间的相似度是由这里的两个不同簇中的距离最近的数据点的数据点对的相似度来定义的。在分裂的层次聚类中，簇根据某个准则进行分裂，例如这个簇中的A和B之间的距离是这个簇中所有样本间距离最远的一对，分类时以样本A和B作为新的簇，其他的样本根据距离A和B的远近划分到对应的簇中。 特点 用户需要提供所希望得到的聚类的单个数量和阈值作为结束条件，而这个条件对于复杂数据来说是难以判定的。 实现原理简单，但是偶尔会遇见合并或分裂点的抉择的困难。 已形成的处理就不能撤销，两个聚类之间也不能交换对象。 不具有很好的伸缩性，因为分类或合并时的决策需要经过检测和估算大量的对象和簇。 层次聚类算法由于使用距离矩阵，时间和空间复杂度都很高O(N^2),几乎不能在大数据集上使用。 层次聚类算法只处理符合某静态模型的簇忽略了不同簇间的信息而且忽略了粗剪的互联性（粗剪距离较近的数据对的多少）和近似度（簇间对数据对的相似度）。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习初涉--k近邻算法]]></title>
      <url>%2F2017%2F05%2F10%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%87%BA%E6%B6%89-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%2F</url>
      <content type="text"><![CDATA[在此对k近邻算法做一个简单的总结： 原理K-近邻方法通过计算每个训练样例到待分类样品的距离，取和待分类样品距离最近的k个训练样例，k个样品中哪个类别的训练样例占多数，则待分类原组就属于哪个类别。在实践中往往通过若干次实验来确定K值，取分类误差率最小的K值。 特点KNN方法主要依靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别，因此对于类域的交叉或者重叠较多的待分类样本集来说，KNN方法更为合适。不足之处是计算量较大，因为对每一个待分类的样本都要计算它到全体已知样本的距离，才能求得它的K个最邻近点。 改进:对于计算量大的问题目前常用的解决办法是事先对已知样本点进行剪辑，实现去除对分类作用不大的样本。对样本进行组织与整理，分群分层，尽可能地将计算压缩在接近测试样本领域的小范围内。 总的来说，算法的适应性较强，尤其适用于样本容量较大的自动分类问题，而那些样本容量较小的分类问题采用这种算法比较容易产生误分。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习初涉—-决策树分类]]></title>
      <url>%2F2017%2F05%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%B6%89%E2%80%94-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%2F</url>
      <content type="text"><![CDATA[决策树分类我在csdn上进行过介绍，传送门：决策树分类算法http://blog.csdn.net/u013668852/article/details/52935169]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[动态规划-DAG-硬币问题]]></title>
      <url>%2F2017%2F04%2F17%2F%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-DAG-%E7%A1%AC%E5%B8%81%E9%97%AE%E9%A2%98%2F</url>
      <content type="text"><![CDATA[题目：有n种硬币，面值分别为V1,V2,…Vn,每种都有无限多。给定非负整数S，可以选用多少个硬币，使得面值之和恰好为S？输出硬币数目的最小值和最大值！ &emsp;&emsp;如果我们有面值为1元、3元和5元的硬币若干枚，如何用最少的硬币凑够11元？ (表面上这道题可以用贪心算法，但贪心算法无法保证可以求出解，比如1元换成2元的时候) &emsp;&emsp;首先我们思考一个问题，如何用最少的硬币凑够i元(i&lt;11)？为什么要这么问呢？两个原因：1.当我们遇到一个大问题时，总是习惯把问题的规模变小，这样便于分析讨论。2.这个规模变小后的问题和原来的问题是同质的，除了规模变小，其它的都是一样的，本质上它还是同一个问题(规模变小后的问题其实是原问题的子问题)。 &emsp;&emsp;好了，让我们从最小的i开始吧。当i=0，即我们需要多少个硬币来凑够0元。由于1，3，5都大于0，即没有比0小的币值，因此凑够0元我们最少需要0个硬币。这时候我们发现用一个标记来表示这句“凑够0元我们最少需要0个硬币。 &emsp;&emsp;那么， 我们用d(i)=j来表示凑够i元最少需要j个硬币。于是我们已经得到了d(0)=0，表示凑够0元最小需要0个硬币。当i=1时，只有面值为1元的硬币可用，因此我们拿起一个面值为1的硬币，接下来只需要凑够0元即可，而这个是已经知道答案的，即d(0)=0。所以，d(1)=d(1-1)+1=d(0)+1=0+1=1。 当i=2时， 仍然只有面值为1的硬币可用，于是我拿起一个面值为1的硬币，接下来我只需要再凑够2-1=1元即可(记得要用最小的硬币数量)，而这个答案也已经知道了。所以d(2)=d(2-1)+1=d(1)+1=1+1=2。 &emsp;&emsp;一直到这里，你都可能会觉得，好无聊，感觉像做小学生的题目似的。因为我们一直都只能操作面值为1的硬币！耐心点，让我们看看i=3时的情况。当i=3时，我们能用的硬币就有两种了：1元的和3元的(5元的仍然没用，因为你需要凑的数目是3元！5元太多了亲)。既然能用的硬币有两种，我就有两种方案。如果我拿了一个1元的硬币，我的目标就变为了:凑够3-1=2元需要的最少硬币数量。即d(3)=d(3-1)+1=d(2)+1=2+1=3。这个方案说的是，我拿3个1元的硬币；第二种方案是我拿起一个3元的硬币，我的目标就变成：凑够3-3=0元需要的最少硬币数量。即d(3)=d(3-3)+1=d(0)+1=0+1=1. &emsp;&emsp;这个方案说的是，我拿1个3元的硬币。好了，这两种方案哪种更优呢？ 记得我们可是要用最少的硬币数量来凑够3元的。所以， 选择d(3)=1，怎么来的呢？ &emsp;&emsp;具体是这样得到的：d(3)=min{d(3-1)+1, d(3-3)+1}。 &emsp;&emsp;上文中d(i)表示凑够i元需要的最少硬币数量，我们将它定义为该问题的”状态”，这个状态是怎么找出来的呢？我在另一篇文章中写过：根据子问题定义状态。你找到子问题，状态也就浮出水面了。 &emsp;&emsp;最终我们要求解的问题，可以用这个状态来表示：d(11)，即凑够11元最少需要多少个硬币。 那状态转移方程是什么呢？既然我们用d(i)表示状态，那么状态转移方程自然包含d(i)， 上文中包含状态d(i)的方程是：d(3)=min{d(3-1)+1,d(3-3)+1}。没错，它就是状态转移方程，描述状态之间是如何转移的。当然，我们要对它抽象一下，d(i)=min{ d(i-vj)+1 }，其中i-vj &gt;=0，vj表示第j个硬币的面值; &emsp;&emsp;有了状态和状态转移方程，这个问题基本上也就解决了。 123456789101112131415161718192021222324252627282930313233#include&lt;cstdio&gt; #include&lt;cstring&gt; int d[1000],v[10]; int max(int a, int b) &#123; return a &gt; b ? a : b; &#125; int dpmax(int s,int n) &#123; if(s&lt;0) return -2&lt;&lt;30; if (d[s] != -1) return d[s]; d[s] = -2&lt;&lt;30;//用于设定不能走到终点的路肯定小于能走到终点的路 for (int i = 0;i &lt; n;i++) if (s &gt;= v[i])//等号很关键 //根据公式：max&#123;dpmax(s-v[i])+1&#125;,代码max(d[s], dpmax(s - v[i], n)+1)中的d[s]记录的是上一个值 d[s] = max(d[s], dpmax(s - v[i], n)+1); return d[s]; &#125; int main() &#123; int n, s, i; while (scanf("%d%d", &amp;n, &amp;s) != EOF) &#123; for (i = 0;i &lt; n;i++) scanf("%d", &amp;v[i]); memset(d, -1, sizeof d); d[0] = 0;//用于辨别该路能否走到终点 printf("%d\n", dpmax(s, n)); &#125; return 0; &#125; 解答转载自http://www.cnblogs.com/sunTin/p/6674945.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[maven打造可执行war包]]></title>
      <url>%2F2017%2F04%2F07%2Fmaven%E6%89%93%E9%80%A0%E5%8F%AF%E6%89%A7%E8%A1%8Cjar%E5%8C%85%2F</url>
      <content type="text"><![CDATA[在开发java Web时，有时我们会使用嵌入式jetty来运行，项目完成后，如果能够直接运行war包从而启动jetty来运行war包那就非常完美了，本文将讲解如何在项目中整合jetty 9，并构造可执行的war包（打包前和打包后都能随时启动）。1.首先添加jetty 9的依赖（本文暂时只用到了jetty的以下依赖，读者根据自己的项目需要增加）12345678910&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-server&lt;/artifactId&gt; &lt;version&gt;9.2.7.v20150116&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-webapp&lt;/artifactId&gt; &lt;version&gt;9.2.7.v20150116&lt;/version&gt;&lt;/dependency&gt; 2.项目中使用jetty 9。 首先我封装了自己的JettyServer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public class EmbeddedServer &#123; //public static final Logger logger = LoggerFactory.getLogger(EmbeddedServer.class); // private static final int DEFAULT_BUFFER_SIZE = 16192; protected final Server server = new Server(); public EmbeddedServer(int port,String path) throws IOException&#123; this(port,path,false,null); &#125; /** * use war to start * @param port * @param isWar * @param warPath * @throws IOException */ public EmbeddedServer(int port,boolean isWar,String warPath) throws IOException&#123; this(port,null,isWar,warPath); &#125; private EmbeddedServer(int port, String path,boolean isWar,String warPath) throws IOException &#123; Connector connector = getConnector(port); server.addConnector(connector); WebAppContext application = getWebAppContext(path,isWar,warPath); server.setHandler(application); server.setStopAtShutdown(true); &#125; protected WebAppContext getWebAppContext(String path,boolean isWar,String warPath) &#123; WebAppContext application; if(isWar)&#123; application=new WebAppContext(); application.setWar(warPath); return application; &#125;else&#123; application = new WebAppContext(path, "/"); application.setConfigurationDiscovered(true); application.setParentLoaderPriority(true); application.setClassLoader(Thread.currentThread().getContextClassLoader()); return application; &#125; &#125; protected Connector getConnector(int port) throws IOException &#123; HttpConfiguration http_config = new HttpConfiguration(); // this is to enable large header sizes when Kerberos is enabled with AD //final int bufferSize = getBufferSize(); //http_config.setResponseHeaderSize(bufferSize); //http_config.setRequestHeaderSize(bufferSize); ServerConnector connector = new ServerConnector(server, new HttpConnectionFactory(http_config)); connector.setPort(port); connector.setHost("0.0.0.0"); server.addConnector(connector); return connector; &#125; /*protected Integer getBufferSize() &#123; try &#123; Configuration configuration = ApplicationProperties.get(); return configuration.getInt("sysimple.jetty.request.buffer.size", DEFAULT_BUFFER_SIZE); &#125; catch (Exception e) &#123; // do nothing &#125; return DEFAULT_BUFFER_SIZE; &#125;*/ public void start() throws Exception &#123; server.start(); //logger.info("********************************************************"); //logger.info("The SySimple Has Started !!!"); server.join(); &#125; public void stop() &#123; try &#123; server.stop(); &#125; catch (Exception e) &#123; //logger.warn("Error during shutdown", e); &#125; &#125;&#125; 接着可以使用封装好的EmbeddedServer来启动war 12345678910111213141516171819public class StartWeb&#123; private static EmbeddedServer embeddedServer; public static void main(String[] args)&#123; //Start web server int port=3000； try&#123; if(args.length==0)&#123; //该方式能够在开发时快速启动 embeddedServer=new EmbeddedServer(port, "src/main/webapp"); &#125;else&#123; //传入war包的路径，该方法能够在打包完成后启动该war包 embeddedServer=new EmbeddedServer(port, true, args[0]); &#125; embeddedServer.start(); &#125;catch(Exception e)&#123; System.exit(0); &#125; &#125;&#125; 注意：打包后如果需要启动war包，需要使用如下的这种批处理命令来启动： 以批处理命令（start.bat）和server.war在同级目录下为例：（以下是start.bat的内容） @echo off set bat_dir=%~dp0 java -jar %bat_dir%/web.war %bat_dir%/web.war 读者可以考虑在代码中得到war包的路径，这样可以在启动时省去传参。 下面是最重要的：使用Maven构建可执行war包 总的来说可执行war包是将war包的结构仿照jar包的结构进行改变，第一个是需要在manifest中标记出主方法，第二个是编译后的代码（包，而非.class）必须放在war包的最外层，最后要能够找到项目的依赖。 ①标记主方法 通过maven-war-plugin在manifest中标记主方法入口 1234567891011&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;org.bit.linc.web.commons.StartWeb&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/plugin&gt; ②拷贝（也可以移动）web的所有的代码到war包最外层（使用maven-antrun-plugin） 12345678910111213141516171819202122&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;main-class-placement&lt;/id&gt; &lt;phase&gt;prepare-package&lt;/phase&gt; &lt;configuration&gt; &lt;target&gt; &lt;copy todir="$&#123;project.build.directory&#125;/$&#123;project.artifactId&#125;/"&gt; &lt;fileset dir="$&#123;project.build.directory&#125;/classes/"&gt; &lt;include name="**/*.*" /&gt; &lt;/fileset&gt; &lt;/copy&gt; &lt;/target&gt; &lt;/configuration&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; ③ 标记所有依赖的位置（将代码拷贝到war最外层后，会出现依赖的类都找不到的情况，因此需要让war包能够查找到这些依赖） 将maven-war-plugin更改为如下内容： 12345678910111213&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;org.bit.linc.web.commons.StartWeb&lt;/mainClass&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;classpathPrefix&gt;WEB-INF/lib&lt;/classpathPrefix&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/plugin&gt; 现在可以构建可执行war包了。 以笔者的项目为例：构建的war包中META-INF/MANIFEST.MF会变成如下内容： Manifest-Version: 1.0 Built-By: wubo Build-Jdk: 1.7.0_17 Class-Path: WEB-INF/lib/commons-0.0.2.jar WEB-INF/lib/commons-configur ation-1.8.jar WEB-INF/lib/commons-lang-2.6.jar WEB-INF/lib/commons-lo gging-1.1.1.jar WEB-INF/lib/slf4j-api-1.7.7.jar WEB-INF/lib/slf4j-log 4j12-1.7.7.jar WEB-INF/lib/log4j-1.2.17.jar WEB-INF/lib/plugins-0.0.2 .jar WEB-INF/lib/clusters-0.0.2.jar WEB-INF/lib/monitors-0.0.2.jar WE B-INF/lib/jetty-server-9.2.7.v20150116.jar WEB-INF/lib/javax.servlet- api-3.1.0.jar WEB-INF/lib/jetty-http-9.2.7.v20150116.jar WEB-INF/lib/ jetty-util-9.2.7.v20150116.jar WEB-INF/lib/jetty-io-9.2.7.v20150116.j ar WEB-INF/lib/jetty-webapp-9.2.7.v20150116.jar WEB-INF/lib/jetty-xml -9.2.7.v20150116.jar WEB-INF/lib/jetty-servlet-9.2.7.v20150116.jar WE B-INF/lib/jetty-security-9.2.7.v20150116.jar WEB-INF/lib/gson-2.3.1.j ar Created-By: Apache Maven 3.3.9 Main-Class: org.bit.linc.web.commons.StartWeb Archiver-Version: Plexus Archiver 其中的Class-Path和Main-Class均已经改变。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习初涉--贝叶斯分类]]></title>
      <url>%2F2017%2F04%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%B6%89-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%2F</url>
      <content type="text"><![CDATA[贝叶斯定理 &emsp;&emsp;这个定理解决了现实生活里经常遇到的问题：已知某条件概率，如何得到两个事件交换后的概率，也就是在已知P(A|B)的情况下如何求得P(B|A)。这里先解释什么是条件概率： &emsp;&emsp;P(A|B) 表示事件B已经发生的前提下，事件A发生的概率，叫做事件B发生下事件A的条件概率。其基本求解公式为：。 &emsp;&emsp;贝叶斯定理之所以有用，是因为我们在生活中经常遇到这种情况：我们可以很容易直接得出P(A|B)，P(B|A)则很难直接得出，但我们更关心P(B|A)，贝叶斯定理就为我们打通从P(A|B)获得P(B|A)的道路。 &emsp;&emsp;下面不加证明地直接给出贝叶斯定理： &emsp;&emsp; 朴素贝叶斯分类朴素贝叶斯分类的原理与流程 &emsp;&emsp;朴素贝叶斯分类是一种十分简单的分类算法，叫它朴素贝叶斯分类是因为这种方法的思想真的很朴素，朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。通俗来说，就好比这么个道理，你在街上看到一个黑人，我问你你猜这哥们哪里来的，你十有八九猜非洲。为什么呢？因为黑人中非洲人的比率最高，当然人家也可能是美洲人或亚洲人，但在没有其它可用信息下，我们会选择条件概率最大的类别，这就是朴素贝叶斯的思想基础。 &emsp;&emsp;朴素贝叶斯分类的正式定义如下：1、设 为一个待分类项，而每个a为x的一个特征属性。2、有类别集合3、计算4、如果，则。 &emsp;&emsp;那么现在的关键就是如何计算第3步中的各个条件概率。我们可以这么做：1、找到一个已知分类的待分类项集合，这个集合叫做训练样本集。2、统计得到在各类别下各个特征属性的条件概率估计。即3、如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有：根据上述分析，朴素贝叶斯分类的流程可以由下图表示（暂时不考虑验证）： 可以看到，整个朴素贝叶斯分类分为三个阶段： &emsp;&emsp;第一阶段——准备工作阶段，这个阶段的任务是为朴素贝叶斯分类做必要的准备，主要工作是根据具体情况确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本集合。这一阶段的输入是所有待分类数据，输出是特征属性和训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。 &emsp;&emsp;第二阶段——分类器训练阶段，这个阶段的任务就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计，并将结果记录。其输入是特征属性和训练样本，输出是分类器。这一阶段是机械性阶段，根据前面讨论的公式可以由程序自动计算完成。 &emsp;&emsp;第三阶段——应用阶段。这个阶段的任务是使用分类器对待分类项进行分类，其输入是分类器和待分类项，输出是待分类项与类别的映射关系。这一阶段也是机械性阶段，由程序完成。 估计类别下特征属性划分的条件概率及Laplace校准 &emsp;&emsp;这一节讨论P(a|y)的估计。 &emsp;&emsp;由上文看出，计算各个划分的条件概率P(a|y)是朴素贝叶斯分类的关键性步骤，当特征属性为离散值时，只要很方便的统计训练样本中各个划分在每个类别中出现的频率即可用来估计P(a|y)，下面重点讨论特征属性是连续值的情况。 &emsp;&emsp;当特征属性为连续值时，通常假定其值服从高斯分布（也称正态分布）。即： &emsp;&emsp;而 &emsp;&emsp;因此只要计算出训练样本中各个类别中此特征项划分的各均值和标准差，代入上述公式即可得到需要的估计值。均值与标准差的计算在此不再赘述。 &emsp;&emsp;另一个需要讨论的问题就是当P(a|y)=0怎么办，当某个类别下某个特征项划分没有出现时，就是产生这种现象，这会令分类器质量大大降低。为了解决这个问题，我们引入Laplace校准，它的思想非常简单，就是对没类别下所有划分的计数加1，这样如果训练样本集数量充分大时，并不会对结果产生影响，并且解决了上述频率为0的尴尬局面。 朴素贝叶斯分类器的应用一、病人分类的例子 &emsp;&emsp;某个医院早上收了六个门诊病人，如下表。 1234567症状 职业 疾病打喷嚏 护士 感冒 打喷嚏 农夫 过敏 头痛 建筑工人 脑震荡 头痛 建筑工人 感冒 打喷嚏 教师 感冒 头痛 教师 脑震荡 &emsp;&emsp;现在又来了第七个病人，是一个打喷嚏的建筑工人。请问他患上感冒的概率有多大？根据贝叶斯定理： 1P(A|B) = P(B|A) P(A) / P(B) &emsp;&emsp;可得 123P(感冒|打喷嚏x建筑工人) = P(打喷嚏x建筑工人|感冒) x P(感冒) / P(打喷嚏x建筑工人) &emsp;&emsp;假定”打喷嚏”和”建筑工人”这两个特征是独立的，因此，上面的等式就变成了 123P(感冒|打喷嚏x建筑工人) = P(打喷嚏|感冒) x P(建筑工人|感冒) x P(感冒) / P(打喷嚏) x P(建筑工人) &emsp;&emsp;这是可以计算的。 123P(感冒|打喷嚏x建筑工人) = 0.66 x 0.33 x 0.5 / 0.5 x 0.33 = 0.66 &emsp;&emsp;因此，这个打喷嚏的建筑工人，有66%的概率是得了感冒。同理，可以计算这个病人患上过敏或脑震荡的概率。比较这几个概率，就可以知道他最可能得什么病。这就是贝叶斯分类器的基本方法：在统计资料的基础上，依据某些特征，计算各个类别的概率，从而实现分类。 二、账号分类的例子&emsp;&emsp;根据某社区网站的抽样统计，该站10000个账号中有89%为真实账号（设为C0），11%为虚假账号（设为C1）。 12C0 = 0.89 C1 = 0.11 &emsp;&emsp;接下来，就要用统计资料判断一个账号的真实性。假定某一个账号有以下三个特征： 123456F1: 日志数量/注册天数 F2: 好友数量/注册天数 F3: 是否使用真实头像（真实头像为1，非真实头像为0） F1 = 0.1 F2 = 0.2 F3 = 0 &emsp;&emsp;请问该账号是真实账号还是虚假账号？&emsp;&emsp;方法是使用朴素贝叶斯分类器，计算下面这个计算式的值。 1P(F1|C)P(F2|C)P(F3|C)P(C) &emsp;&emsp;虽然上面这些值可以从统计资料得到，但是这里有一个问题：F1和F2是连续变量，不适宜按照某个特定值计算概率。&emsp;&emsp;一个技巧是将连续值变为离散值，计算区间的概率。比如将F1分解成[0, 0.05]、(0.05, 0.2)、[0.2, +∞]三个区间，然后计算每个区间的概率。在我们这个例子中，F1等于0.1，落在第二个区间，所以计算的时候，就使用第二个区间的发生概率。&emsp;&emsp;根据统计资料，可得： 123P(F1|C0) = 0.5, P(F1|C1) = 0.1 P(F2|C0) = 0.7, P(F2|C1) = 0.2 P(F3|C0) = 0.2, P(F3|C1) = 0.9 &emsp;&emsp;因此， 123456P(F1|C0) P(F2|C0) P(F3|C0) P(C0) = 0.5 x 0.7 x 0.2 x 0.89 = 0.0623 P(F1|C1) P(F2|C1) P(F3|C1) P(C1) = 0.1 x 0.2 x 0.9 x 0.11 = 0.00198 &emsp;&emsp;可以看到，虽然这个用户没有使用真实头像，但是他是真实账号的概率，比虚假账号高出30多倍，因此判断这个账号为真。三、性别分类的例子&emsp;&emsp;下面是一组人类身体特征的统计资料。 123456789性别 身高（英尺） 体重（磅） 脚掌（英寸）男 6 180 12 男 5.92 190 11 男 5.58 170 12 男 5.92 165 10 女 5 100 6 女 5.5 150 8 女 5.42 130 7 女 5.75 150 9 &emsp;&emsp;已知某人身高6英尺、体重130磅，脚掌8英寸，请问该人是男是女？&emsp;&emsp;根据朴素贝叶斯分类器，计算下面这个式子的值。 1P(身高|性别) x P(体重|性别) x P(脚掌|性别) x P(性别) &emsp;&emsp;这里的困难在于，由于身高、体重、脚掌都是连续变量，不能采用离散变量的方法计算概率。而且由于样本太少，所以也无法分成区间计算。怎么办？&emsp;&emsp;这时，可以假设男性和女性的身高、体重、脚掌都是正态分布，通过样本计算出均值和方差，也就是得到正态分布的密度函数。有了密度函数，就可以把值代入，算出某一点的密度函数的值。&emsp;&emsp;比如，男性的身高是均值5.855、方差0.035的正态分布。所以，男性的身高为6英尺的概率的相对值等于1.5789（大于1并没有关系，因为这里是密度函数的值，只用来反映各个值的相对可能性）。 &emsp;&emsp;有了这些数据以后，就可以计算性别的分类了。 1234P(身高=6|男) x P(体重=130|男) x P(脚掌=8|男) x P(男) = 6.1984 x e-9P(身高=6|女) x P(体重=130|女) x P(脚掌=8|女) x P(女) = 5.3778 x e-4 &emsp;&emsp;可以看到，女性的概率比男性要高出将近10000倍，所以判断该人为女性。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[maven多级项目使用slf4j+log4j，以及自定义配置文件路径]]></title>
      <url>%2F2017%2F03%2F23%2Fmaven%E5%A4%9A%E7%BA%A7%E9%A1%B9%E7%9B%AE%E4%BD%BF%E7%94%A8slf4j%2Blog4j%E4%BB%A5%E5%8F%8A%E8%87%AA%E5%AE%9A%E4%B9%89%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84%20-%20%E5%89%AF%E6%9C%AC%2F</url>
      <content type="text"><![CDATA[我的maven多级结构如下： sysimple |--integration |--commons |--pom.xml |--plugins |--pom.xml |--web |--pom.xml |--pom.xml 其中依赖情况是： web依赖于commons和plugins。plugins依赖于commons。integration中定义了打包的方法与资源文件。 首先在sysimple/pom.xml中管理slf4j的版本： 在&lt;dependencyManagement&gt;&lt;/dependencyManagement&gt;中间添加： &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.7&lt;/version&gt; &lt;/dependency&gt; 由于所有的模块均引用commons，因此只需要在commons中添加slf4j的依赖即可： &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;/dependency&gt; 下面即可使用slf4j，在需要使用的地方以如下方式使用： 123456public class StartWeb &#123; private static final Logger logger = LoggerFactory.getLogger(StartWeb.class); public static void main(String[] args)&#123; logger.info("this is a example"); &#125;&#125; 默认情况下，slf4j-log4j会在src/main/java中查找log4j.properties，如果需要指定配置文件的位置，需要在启动时手动加入Jvm的参数，我的例子中添加了-Dlog4j.configuration=file:../integration/conf/sysimple-log4j.properties。在使用绝对路径时是不需要使用file:的，linux端也不需要file:。在运行的时候，slf4j会根据你指定的路径去加载配置文件。配置文件的内容我给出以下例子， 读者可以另行查找配置文件的格式： log4j.rootLogger=INFO,system.out log4j.appender.system.out=org.apache.log4j.ConsoleAppender log4j.appender.system.out.layout=org.apache.log4j.PatternLayout log4j.appender.system.out.layout.ConversionPattern=SysimpleServer Logger--&gt;%5p{%F:%L}-%m%n log4j.logger.thisProject.file=INFO,thisProject.file.out log4j.appender.thisProject.file.out=org.apache.log4j.DailyRollingFileAppender log4j.appender.thisProject.file.out.File=../integration/logs/sysimple-logs.log log4j.appender.thisProject.file.out.layout=org.apache.log4j.PatternLayout]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[jvm垃圾收集算法]]></title>
      <url>%2F2017%2F03%2F15%2Fjvm%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95%2F</url>
      <content type="text"><![CDATA[来源：JVM虚拟机 标记-清除法最基础的收集算法是“标记-清除”（Mark-Sweep）算法，如同它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象，它的标记过程其实在前一节讲述对象标记判定时已经介绍过了。 之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其不足进行改进而得到的。 它的主要不足有两个：==一个是效率问题，标记和清除两个过程的效率都不高；另一个是空间问题，标记清除之后会产生大量不连续的内存碎片==，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 复制算法为了解决效率问题，一种称为“复制”（Copying）的收集算法出现了，它将可用==内存按容量划分为大小相等的两块，每次只使用其中的一块==。 ==当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉==。 这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。 只是这种算法的代价是将内存缩小为了原来的一半，未免太高了一点。 复制算法的执行过程如图3-3所示。 现在的商业虚拟机都采用这种收集算法来回收新生代，IBM公司的专门研究表明，新生代中的对象98%是“朝生夕死”的。所以并不需要按照1:1的比例来划分内存空间，==而是将内存分为一块较大的Eden空间和两块较小的Survivor空间==，每次使用Eden和其中一块Survivor[1]。当回收时，将Eden和Survivor中还存活着的对象一次性地复制到另外一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。 HotSpot虚拟机默认Eden和Survivor的大小比例是8:1，也就是每次新生代中可用内存空间为整个新生代容量的90%（80%+10%），只有10%的内存会被“浪费”。 当然，98%的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于10%的对象存活，当Survivor空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。 标记整理算法复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会变低。 更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 分代收集算法当前商业虚拟机的垃圾收集都采用“分代收集”（Generational Collection）算法，这种算法并没有什么新的思想，==只是根据对象存活周期的不同将内存划分为几块==。 一般是把Java堆分为新生代和老年代，这样就可以==根据各个年代的特点采用最适当的收集算法==。 在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。 而老年代中因为对象存活率高、 没有额外空间对它进行分配担保，就必须使用“标记—清理”或者“标记—整理”算法来进行回收。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[java对象的创建过程]]></title>
      <url>%2F2017%2F03%2F15%2Fjava%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%88%9B%E5%BB%BA%E8%BF%87%E7%A8%8B%2F</url>
      <content type="text"><![CDATA[对象的创建过程 来源：JVM虚拟机 Java是一门面向对象的编程语言，在Java程序运行过程中无时无刻都有对象被创建出来。在语言层面上，创建对象（例如克隆、反序列化）通常仅仅是一个new关键字而已，而在虚拟机中，对象（文中讨论的对象限于普通Java对象，不包括数组和Class对象等）的创建又是怎样一个过程呢？虚拟机遇到一条new指令时，==首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过==。 如果没有，那必须先执行相应的类加载过程。 ==在类加载检查通过后，接下来虚拟机将为新生对象分配内存==。==对象所需内存的大小在类加载完成后便可完全确定==（如何确定将在2.3.2节中介绍），为对象分配空间的任务等同于把一块确定大小的内存从Java堆中划分出来。++假设Java堆中内存是绝对规整的++，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间那边挪动一段与对象大小相等的距离，这种分配方式称为“++==指针碰撞==++”（Bump the Pointer）。如果Java堆中的内存并++不是规整的++，已使用的内存和空闲的内存相互交错，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式称为“==空闲列表==”（Free List）。 选择哪种分配方式由Java堆是否规整决定，而Java堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 因此，在使用Serial、 ParNew等带Compact过程的收集器时，系统采用的分配算法是指针碰撞，而使用CMS这种基于Mark-Sweep算法的收集器时，通常采用空闲列表。 除如何划分可用空间之外，还有另外一个需要考虑的问题是对象创建在虚拟机中是非常频繁的行为，即使是仅仅修改一个指针所指向的位置，在并发情况下也并不是线程安全的，可能出现正在给对象A分配内存，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存的情况。解决这个问题有两种方案，==一种是对分配内存空间的动作进行同步处理==——实际上虚拟机采用CAS配上失败重试的方式保证更新操作的原子性；==另一种是把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer,TLAB）==。哪个线程要分配内存，就在哪个线程的TLAB上分配，只有TLAB用完并分配新的TLAB时，才需要同步锁定。虚拟机是否使用TLAB，可以通过-XX：+/-UseTLAB参数来设定。 内存分配完成后，==虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），如果使用TLAB，这一工作过程也可以提前至TLAB分配时进行==。 这一步操作保证了对象的实例字段在Java代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 接下来，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、 如何才能找到类的元数据信息、 对象的哈希码、对象的GC分代年龄等信息。 这些信息存放在对象的对象头（Object Header）之中。根据虚拟机当前的运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 关于对象头的具体内容，稍后再做详细介绍。 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从Java程序的视角来看，对象创建才刚刚开始——＜init＞方法还没有执行，所有的字段都还为零。所以，一般来说（由字节码中是否跟随invokespecial指令所决定），==执行new指令之后会接着执行＜init＞方法==，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Storm和Spark streaming对比]]></title>
      <url>%2F2017%2F02%2F28%2FStorm%E5%92%8CSpark-streaming%E5%AF%B9%E6%AF%94%2F</url>
      <content type="text"><![CDATA[Spark Streaming与Storm的应用场景对于Storm来说：1、建议在那种需要纯实时，不能忍受1秒以上延迟的场景下使用，比如实时金融系统，要求纯实时进行金融交易和分析2、此外，如果对于实时计算的功能中，要求可靠的事务机制和可靠性机制，即数据的处理完全精准，一条也不能多，一条也不能少，也可以考虑使用Storm3、如果还需要针对高峰低峰时间段，动态调整实时计算程序的并行度，以最大限度利用集群资源（通常是在小型公司，集群资源紧张的情况），也可以考虑用Storm4、如果一个大数据应用系统，它就是纯粹的实时计算，不需要在中间执行SQL交互式查询、复杂的transformation算子等，那么用Storm是比较好的选择。 对于Spark Streaming来说：1、如果对上述适用于Storm的三点，一条都不满足的实时场景，即，不要求纯实时，不要求强大可靠的事务机制，不要求动态调整并行度，那么可以考虑使用Spark Streaming2、考虑使用Spark Streaming最主要的一个因素，应该是针对整个项目进行宏观的考虑，即，如果一个项目除了实时计算之外，还包括了离线批处理、交互式查询等业务功能，而且实时计算中，可能还会牵扯到高延迟批处理、交互式查询等功能，那么就应该首选Spark生态，用Spark Core开发离线批处理，用Spark SQL开发交互式查询，用Spark Streaming开发实时计算，三者可以无缝整合，给系统提供非常高的可扩展性 Spark Streaming与Storm的优劣分析事实上，Spark Streaming绝对谈不上比Storm优秀。这两个框架在实时计算领域中，都很优秀，只是擅长的细分场景并不相同。Spark Streaming仅仅在吞吐量上比Storm要优秀，而吞吐量这一点，也是历来挺Spark Streaming，贬Storm的人着重强调的。但是问题是，是不是在所有的实时计算场景下，都那么注重吞吐量？不尽然。因此，通过吞吐量说Spark Streaming强于Storm，不靠谱。事实上，Storm在实时延迟度上，比Spark Streaming就好多了，前者是纯实时，后者是准实时。而且，Storm的事务机制、健壮性 / 容错性、动态调整并行度等特性，都要比Spark Streaming更加优秀。Spark Streaming，有一点是Storm绝对比不上的，就是：它位于Spark生态技术栈中，因此Spark Streaming可以和Spark Core、Spark SQL无缝整合，也就意味着，我们可以对实时处理出来的中间数据，立即在程序中无缝进行延迟批处理、交互式查询等操作。这个特点大大增强了Spark Streaming的优势和功能。 Storm和Spark streaming对比 对比点 storm Spark Streaming \实时计算模型&emsp; \纯实时，来一条数据，处理一条数据 \准实时，对一个时间段内的数据收集起来，作为一个RDD，再处理 \实时计算延迟度&emsp; \毫秒级 \秒级 \吞吐量&emsp; \低 \高 \事务机制&emsp; \支持完善 \支持，但不够完善 \健壮性 / 容错性&emsp; \ZooKeeper，Acker，非常强 \Checkpoint，WAL，一般 \动态调整并行度&emsp; \支持 \不支持]]></content>
    </entry>

    
  
  
</search>
