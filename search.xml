<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[机器学习实战--决策树分类]]></title>
      <url>%2F2017%2F06%2F25%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%2F</url>
      <content type="text"><![CDATA[以下是python的决策树实现，采用的是信息增益来选取最好的属性，即 ID3算法：参考机器学习实战，在实践中，给了一点自己的注释，希望能帮助大家理解。关于决策树的讲解，在另一篇博客中我给过介绍，有兴趣的可以看下相关的内容，建议不了解原理的先了解决策树的原理，弄清算法的流程和几个基本概念。决策树分类算法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899# encoding:utf-8from math import logimport operatordef createDataSet(): dataSet = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] labels = ['no surfacing', 'flippers'] return dataSet, labels# 计算熵def calcShannoEnt(dataSet): numEntries = len(dataSet) labelCounts = &#123;&#125; for featVec in dataSet: currentLable = featVec[-1] if currentLable not in labelCounts.keys(): labelCounts[currentLable] = 0 labelCounts[currentLable] += 1 shannoEnt = 0.0 for key in labelCounts: prof = float(labelCounts[key]) / numEntries # 求得P(i) shannoEnt -= prof * log(prof, 2) # 求-log2P(i)的期望值 return shannoEnt# 根据属性的下标和属性的值对数据集进行划分（这个方法和我们给出的数据是高度适配的，即每行的数据最后一个是分类标签，之前的每列代表一个属性，数据集不同的话，处理的过程也不同）def splitDataSet(dataSet, axix, value): retDataSet = [] for featVec in dataSet: if featVec[axix] == value: # 按照指定的列划分数据后，划分后的数据需要去除该列的属性 reducedFeatVec = featVec[:axix] # 不包括axix列 reducedFeatVec.extend(featVec[axix + 1:]) retDataSet.append(reducedFeatVec) return retDataSetdef chooseBestFeatureToSplit(dataSet): numFeatures = len(dataSet[0]) - 1 # 属性的数量 baseEntropy = calcShannoEnt(dataSet) bestInfoGain = 0.0 bestFeature = -1 for i in range(numFeatures): featList = [example[i] for example in dataSet] # 第i个属性，即第i列的所有值（包含重复） uniqueVals = set(featList) # 去重 newEntropy = 0.0 for value in uniqueVals: subDataSet = splitDataSet(dataSet, i, value) # 划分 prob = len(subDataSet) / float(len(dataSet)) # 计算每个类别的概率 newEntropy += prob * calcShannoEnt(subDataSet) # 计算每个类别的熵 infoGain = baseEntropy - newEntropy # 计算根据该属性划分后的信息增益率 if infoGain &gt; bestInfoGain: # 按照决策树划分的原则，会选择信息增益最大的属性进行划分 bestInfoGain = infoGain bestFeature = i return bestFeaturedef majorityCnt(classList): """ 取占据大部分的类别 :param classList: 类别列表 :return: """ classCount = &#123;&#125; for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0]def createTree(dataSet, labels): classList = [example[-1] for example in dataSet] # 得到所有的类别 if classList.count(classList[0]) == len(classList): # 类别完全相同，则停止划分 return classList[0] # 返回类型信息 if len(dataSet[0]) == 1: return majorityCnt(classList) bestFeat = chooseBestFeatureToSplit(dataSet) # 求得最好的划分属性下标 bestFeatLabel = labels[bestFeat] # 最好的划分属性名 myTree = &#123;bestFeatLabel: &#123;&#125;&#125; del labels[bestFeat] featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) for value in uniqueVals: # 使用最好的属性划分后能够得到一些子数据集，对这些数据集继续进行划分 subLabels = labels[:] myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) return myTreeif __name__ == '__main__': myDat, labels = createDataSet() print createTree(myDat,labels) 运行的结果如下： 1&#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125; 那么训练好的决策树如何使用呢？一般而言，训练好的决策树是一种知识，会作为知识存储起来，以便进行分类时直接使用。 下面进行对待分类的数据进行分类： 在上面的代码中加入下面的测试方法： 1234567891011121314# 对测试数据进行分类def classify(inputTree, featLabels, testVec): firstStr = inputTree.keys()[0] # 当前划分的最好属性 secondDict = inputTree[firstStr] featIndex = featLabels.index(firstStr) # 将标签字符串转换为索引 # 层次遍历划分属性对应的划分值，判断测试数据是哪种 for key in secondDict.keys(): if testVec[featIndex] == key: if type(secondDict[key]).__name__ == 'dict': classLabel = classify(secondDict[key], featLabels, testVec) else: classLabel = secondDict[key] break return classLabel 相应的main方法更改为如下内容： 123456if __name__ == '__main__': myDat, labels = createDataSet() templabels = labels[:] mytree = createTree(myDat,templabels) testVec = [1,0] print classify(mytree,labels,testVec) 说明：createTree会更改labels，因此，在使用createTree方法时，需要传入labels复制出的templabels。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习实战--KNN]]></title>
      <url>%2F2017%2F06%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-KNN%2F</url>
      <content type="text"><![CDATA[本文参考机器学习实战一书，使用python具体实现了KNN，从概念到实现与应用。不熟悉KNN的，需要先熟悉下KNN的基本思想，再看本文的代码实现。测试数据地址为：测试数据地址12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091# encoding:utf-8from numpy import *import operatorimport matplotlibimport matplotlib.pyplot as pltdef createDataSet(): group = array([[1.0, 1.1], [1.0, 1.0], [0, 0], [0, 0.1]]) labels = ['A', 'A', 'B', 'B'] return group, labelsdef classify0(intX, dataSet, labels, k): dataSetSize = dataSet.shape[0] ds = tile(intX, (dataSetSize, 1)) diffMat = ds - dataSet sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances ** 0.5 sortedDistIndicies = distances.argsort() classCount = &#123;&#125; for i in range(k): voteIlabel = labels[sortedDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0]def file2matrix(filename, dim2): fr = open(filename) arrayOLines = fr.readlines() numberOfLines = len(arrayOLines) returnMat = zeros((numberOfLines, dim2)) classLabelVector = [] index = 0 for line in arrayOLines: line = line.strip() listFromLine = line.split('\t') returnMat[index, :] = listFromLine[0:dim2] classLabelVector.append(int(listFromLine[-1])) index += 1 return returnMat, classLabelVectordef autoNorm(dataSet): minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals normDataSet = zeros(shape(dataSet)) m = dataSet.shape[0] normDataSet = dataSet - tile(minVals, (m, 1)) normDataSet = normDataSet / tile(ranges, (m, 1)) return normDataSet, ranges, minValsdef datingClassTest(): hoRation = 0.10 datingDataMat, datingLabels = file2matrix('datingTestSet2.txt', 3) normMat, ranges, minVals = autoNorm(datingDataMat) m = normMat.shape[0] numTestVecs = int(m * hoRation) errCount = 0.0 for i in range(numTestVecs): classifierResult = classify0(normMat[i, :], normMat[numTestVecs:m, :], datingLabels[numTestVecs:m], 3) print "分类器结果：%d， 实际结果为：%d" % (classifierResult, datingLabels[i]) if classifierResult != datingLabels[i]: errCount += 1.0 print "err rate：%f" % (errCount / float(numTestVecs)) numTestVecsdef classifyPerson(): resultList = ['not all', 'in small doses', 'in large doses'] percentTats = float(raw_input(u"在游戏上花费的时间占比( )%:")) ffMiles = float(raw_input(u"每年航空的里程数:")) iceCream = float(raw_input(u"每年吃的冰淇淋（升）")) datingDataMat, datingLabels = file2matrix('datingTestSet2.txt', 3) normMat, ranges, minVals = autoNorm(datingDataMat) inArr = array([ffMiles, percentTats, iceCream]) classifiResult = classify0(inArr / ranges, normMat, datingLabels, 3) print "你可能是属于以下这类人：", resultList[classifiResult - 1]if __name__ == '__main__': classifyPerson() # datingClassTest() # fig = plt.figure() # ax = fig.add_subplot(111) # ax.scatter(datingDataMat[:, 1], datingDataMat[:, 2], 15.0 * array(datingLabels), array(datingLabels)) # plt.show()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python函数式编程]]></title>
      <url>%2F2017%2F06%2F11%2Fpython%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%2F</url>
      <content type="text"><![CDATA[Python语言中，用于函数式编程的主要由3个基本函数和1个算子。 基本函数：map()、reduce()、filter() 算子(operator)：lambda Python函数式编程的基本单元lambdalambda这个关键词在很多语言中都存在。简单地说，它可以实现函数创建的功能。 如下便是lambda的两种使用方式。 123func1 = lambda : &lt;expression()&gt;func2 = lambda x : &lt;expression(x)&gt;func3 = lambda x,y : &lt;expression(x,y)&gt; 在第一条语句中，采用lambda创建了一个无参的函数func1。这和下面采用def创建函数的效果是相同的。 12def func1(): &lt;expression()&gt; 第二个语句和第三个语句分别传入一个和两个参数，我想不用解释也就懂了。 需要注意的是，调用func1的时候，虽然不需要传入参数，但是必须要带有括号()，否则返回的只是函数的定义，而非函数执行的结果。这个和在ruby中调用无参函数时有所不同，希望ruby程序员引起注意。 12345&gt;&gt;&gt; func = lambda : 123&gt;&gt;&gt; func&lt;function &lt;lambda&gt; at 0x100f4e1b8&gt;&gt;&gt;&gt; func()123 另外，虽然在上面例子中都将lambda创建的函数赋值给了一个函数名，但这并不是必须的。从下面的例子中大家可以看到，很多时候我们都是直接调用lambda创建的函数，而并没有命名一个函数，这也是我们常听说的匿名函数的由来。 map()map()函数的常见调用形式如下所示： map(func, iterable) map()需要两个必填参数，第一个参数是一个函数名，第二个参数是一个可迭代的对象，如列表、元组等。 map()实现的功能很简单，就是将第二个参数（iterable）中的每一个元素分别传给第一个参数（func），依次执行函数得到结果，并将结果组成一个新的list对象后进行返回。返回结果永远都是一个list。 123&gt;&gt;&gt; double_func = lambda s : s * 2&gt;&gt;&gt; map(double_func, [1,2,3,4,5])[2, 4, 6, 8, 10] 除了传入一个可迭代对象这种常见的模式外，map()还支持传入多个可迭代对象。 map(func, iterable1, iterable2) 在传入多个可迭代对象的情况下，map()会依次从所有可迭代对象中依次取一个元素，组成一个元组列表，然后将元组依次传给func；若可迭代对象的长度不一致，则会以None进行补上。 通过以下示例应该就比较容易理解。 1234567&gt;&gt;&gt; plus = lambda x,y : (x or 0) + (y or 0)&gt;&gt;&gt; map(plus, [1,2,3], [4,5,6])[5, 7, 9]&gt;&gt;&gt; map(plus, [1,2,3,4], [4,5,6])[5, 7, 9, 4]&gt;&gt;&gt; map(plus, [1,2,3], [4,5,6,7])[5, 7, 9, 7] 在上面的例子中，之所以采用x or 0的形式，是为了防止None + int出现异常。 需要注意的是，可迭代对象的个数应该与func的参数个数一致，否则就会出现异常，因为传参个数与函数参数个数不一致了，这个应该比较好理解。 12345&gt;&gt;&gt; plus = lambda x,y : x + y&gt;&gt;&gt; map(plus, [1,2,3])Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: &lt;lambda&gt;() takes exactly 2 arguments (1 given) 另外，map()还存在一种特殊情况，就是func为None。这个时候，map()仍然是从所有可迭代对象中依次取一个元素，组成一个元组列表，然后将这个元组列表作为结果进行返回。 12345678&gt;&gt;&gt; map(None, [1,2,3,4])[1, 2, 3, 4]&gt;&gt;&gt; map(None, [1,2,3,4], [5,6,7,8])[(1, 5), (2, 6), (3, 7), (4, 8)]&gt;&gt;&gt; map(None, [1,2,3,4], [5,6,7])[(1, 5), (2, 6), (3, 7), (4, None)]&gt;&gt;&gt; map(None, [1,2,3,4], [6,7,8,9], [11,12])[(1, 6, 11), (2, 7, 12), (3, 8, None), (4, 9, None)] Reduce()reduce()函数的调用形式如下所示： reduce(func, iterable[, initializer]) reduce()函数的功能是对可迭代对象（iterable）中的元素从左到右进行累计运算，最终得到一个数值。第三个参数initializer是初始数值，可以空置，空置为None时就从可迭代对象（iterable）的第二个元素开始，并将第一个元素作为初始化的结果。 文字描述可能不大清楚，看下reduce()的源码应该就比较清晰了。 1234567891011def reduce(function, iterable, initializer=None): it = iter(iterable) if initializer is None: try: initializer = next(it) except StopIteration: raise TypeError('reduce() of empty sequence with no initial value') accum_value = initializer for x in it: accum_value = function(accum_value, x) return accum_value 再加上如下示例，对reduce()的功能应该就能掌握了 12345&gt;&gt;&gt; plus = lambda x, y : x + y&gt;&gt;&gt; reduce(plus, [1,2,3,4,5])15&gt;&gt;&gt; reduce(plus, [1,2,3,4,5], 10)25 filter()filter()函数的调用形式如下： filter(func, iterable) filter()有且仅有两个参数，第一个参数是一个函数名，第二个参数是一个可迭代的对象，如列表、元组等。 filter()函数的调用形式与map()比较相近，都是将第二个参数（iterable）中的每一个元素分别传给第一个参数（func），依次执行函数得到结果；差异在于，filter()会判断每次执行结果的bool值，并只将bool值为true的筛选出来，组成一个新的列表并进行返回。 123&gt;&gt;&gt; mode2 = lambda x : x % 2&gt;&gt;&gt; filter(mode2, [1,2,3,4,5,6,7,8,9,10])[1, 3, 5, 7, 9] 以上便是Python函数式编程基本单元的核心内容。 接下来，我们就开始尝试采用新学习到的基本单元对命令式编程中的条件控制和循环控制进行转换。 替换条件控制语句在对条件控制进行替换之前，我们先来回顾下Python中对布尔表达式求值时进行的“短路”处理。 什么叫“短路”处理？简单地讲，就是如下两点： 在f(x) and g(y)中，当f(x)为false时，不会再执行g(y)，直接返回false 在f(x) or g(y)中，当f(x)为true时，不会再执行g(y)，直接返回true 结论是显然易现的，就不再过多解释。 那么，对应到条件控制语句，我们不难理解，如下条件控制语句和表达式是等价的。 1234# flow control statementif &lt;cond1&gt;: func1()elif &lt;cond2&gt;: func2()else: func3() 12# Equivalent "short circuit" expression(&lt;cond1&gt; and func1()) or (&lt;cond2&gt; and func2()) or (func3()) 通过这个等价替换，我们就去除掉了if/elif/else关键词，将条件控制语句转换为一个表达式。那这个表达式和函数式编程有什么关系呢？ 这时我们回顾上面讲过的lambda，会发现lambda算子返回的就是一个表达式。 基于这一点，我们就可以采用lambda创建如下函数。 12345678910&gt;&gt;&gt; pr = lambda s:s&gt;&gt;&gt; print_num = lambda x: (x==1 and pr("one")) .... or (x==2 and pr("two")) .... or (pr("other"))&gt;&gt;&gt; print_num(1)'one'&gt;&gt;&gt; print_num(2)'two'&gt;&gt;&gt; print_num(3)'other' 通过函数调用的结果可以看到，以上函数实现的功能与之前的条件控制语句实现的功能完全相同。 到这里，我们就实现了命令式条件控制语句向函数式语句的转换。并且这个转换的方法是通用的，所有条件控制语句都可以采用这种方式转换为函数式语句。 替换循环控制语句接下来我们再看循环控制语句的转换。在Python中，循环控制是通过for和while这两种方式实现的。 替换for循环for循环语句的替换十分简单，采用map()函数就能轻松实现。这主要是因为for语句和map()原理相同，都是对可迭代对象里面的每一个元素进行操作，因此转换过程比较自然。 12345# statement-based for loopfor e in lst: func(e) # Equivalent map()-based loopmap(func, lst) 12345678910&gt;&gt;&gt; square = lambda x : x * x&gt;&gt;&gt; for x in [1,2,3,4,5]: square(x)...1491625&gt;&gt;&gt; map(square, [1,2,3,4,5])[1, 4, 9, 16, 25] 替换while循环了解了while的循环替代后，我的直觉是以后应该不会用到吧，可能是我目前只是一个python小白，目前这块就不贴出来了，网上有很多教程，譬如Python的函数式编程，从入门到⎡放弃⎦]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[动态规划:word-break]]></title>
      <url>%2F2017%2F06%2F09%2F%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-word-break%2F</url>
      <content type="text"><![CDATA[题目描述Given a string s and a dictionary of words dict, determine if s can be segmented into a space-separated sequence of one or more dictionary words.For example, givens =”leetcode”,dict =[“leet”, “code”].Return true because”leetcode”can be segmented as”leet code”. 解题123456789101112131415161718192021222324252627282930import java.util.HashMap;import java.util.HashSet;import java.util.Set;public class Solution &#123; public boolean wordBreak(String s, Set&lt;String&gt; dict) &#123; HashMap&lt;String,Boolean&gt; findString=new HashMap&lt;&gt;(); return dfs(s,findString,dict); &#125; public boolean dfs(String s,HashMap&lt;String,Boolean&gt; hashMap,Set&lt;String&gt; dict)&#123; if (hashMap.containsKey(s)) return hashMap.get(s); if (s.equals(""))&#123; return true; &#125; boolean find=false; int len=s.length(); for (int i=1;i&lt;=len;i++)&#123; String header=s.substring(0,i); if (dict.contains(header))&#123; boolean child=dfs(s.substring(i,s.length()),hashMap,dict); if (child==true)&#123; find=true; break; &#125; &#125; &#125; hashMap.put(s,find); return find; &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[pandas入门]]></title>
      <url>%2F2017%2F06%2F07%2Fpandas%E5%85%A5%E9%97%A8%2F</url>
      <content type="text"><![CDATA[http://pandas.pydata.org/pandas-docs/stable/10min.html 习惯上,我们做以下导入 123In [1]: import pandas as pdIn [2]: import numpy as npIn [3]: import matplotlib.pyplot as plt 创建对象使用传递的值列表序列创建序列, 让pandas创建默认整数索引 12345678910In [4]: s = pd.Series([1,3,5,np.nan,6,8])In [5]: sOut[5]: 0 11 32 53 NaN4 65 8dtype: float64 使用传递的numpy数组创建数据帧,并使用日期索引和标记列.1234567891011121314In [10]: df2 = pd.DataFrame(&#123; 'A' : 1., ....: 'B' : pd.Timestamp('20130102'), ....: 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), ....: 'D' : np.array([3] * 4,dtype='int32'), ....: 'E' : pd.Categorical(["test","train","test","train"]), ....: 'F' : 'foo' &#125;) ....: In [11]: df2Out[11]: A B C D E F0 1 2013-01-02 1 3 test foo1 1 2013-01-02 1 3 train foo2 1 2013-01-02 1 3 test foo3 1 2013-01-02 1 3 train foo 所有明确类型 123456789In [12]: df2.dtypesOut[12]: A float64B datetime64[ns]C float32D int32E categoryF objectdtype: object 如果你这个正在使用IPython，标签补全列名（以及公共属性）将自动启用。这里是将要完成的属性的子集： 123456789101112131415161718192021222324In [13]: df2.&lt;TAB&gt;df2.A df2.boxplotdf2.abs df2.Cdf2.add df2.clipdf2.add_prefix df2.clip_lowerdf2.add_suffix df2.clip_upperdf2.align df2.columnsdf2.all df2.combinedf2.any df2.combineAdddf2.append df2.combine_firstdf2.apply df2.combineMultdf2.applymap df2.compounddf2.as_blocks df2.consolidatedf2.asfreq df2.convert_objectsdf2.as_matrix df2.copydf2.astype df2.corrdf2.at df2.corrwithdf2.at_time df2.countdf2.axes df2.covdf2.B df2.cummaxdf2.between_time df2.cummindf2.bfill df2.cumproddf2.blocks df2.cumsumdf2.bool df2.D 如你所见, 列 A, B, C, 和 D 也是自动完成标签. E 也是可用的; 为了简便起见,后面的属性显示被截断. 查看数据查看帧顶部和底部行 123456789101112131415In [14]: df.head()Out[14]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.087401 In [15]: df.tail(3)Out[15]: A B C D2013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988 显示索引,列,和底层numpy数据 1234567891011121314151617In [16]: df.indexOut[16]: &lt;class 'pandas.tseries.index.DatetimeIndex'&gt;[2013-01-01, ..., 2013-01-06]Length: 6, Freq: D, Timezone: NoneIn [17]: df.columnsOut[17]: Index([u'A', u'B', u'C', u'D'], dtype='object')In [18]: df.valuesOut[18]: array([[ 0.4691, -0.2829, -1.5091, -1.1356], [ 1.2121, -0.1732, 0.1192, -1.0442], [-0.8618, -2.1046, -0.4949, 1.0718], [ 0.7216, -0.7068, -1.0396, 0.2719], [-0.425 , 0.567 , 0.2762, -1.0874], [-0.6737, 0.1136, -1.4784, 0.525 ]]) 描述显示数据快速统计摘要 1234567891011In [19]: df.describe()Out[19]: A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.073711 -0.431125 -0.687758 -0.233103std 0.843157 0.922818 0.779887 0.973118min -0.861849 -2.104569 -1.509059 -1.13563225% -0.611510 -0.600794 -1.368714 -1.07661050% 0.022070 -0.228039 -0.767252 -0.38618875% 0.658444 0.041933 -0.034326 0.461706max 1.212112 0.567020 0.276232 1.071804 转置数据 1234567In [20]: df.TOut[20]: 2013-01-01 2013-01-02 2013-01-03 2013-01-04 2013-01-05 2013-01-06A 0.469112 1.212112 -0.861849 0.721555 -0.424972 -0.673690B -0.282863 -0.173215 -2.104569 -0.706771 0.567020 0.113648C -1.509059 0.119209 -0.494929 -1.039575 0.276232 -1.478427D -1.135632 -1.044236 1.071804 0.271860 -1.087401 0.524988 按轴排序 123456789In [21]: df.sort_index(axis=1, ascending=False)Out[21]: D C B A2013-01-01 -1.135632 -1.509059 -0.282863 0.4691122013-01-02 -1.044236 0.119209 -0.173215 1.2121122013-01-03 1.071804 -0.494929 -2.104569 -0.8618492013-01-04 0.271860 -1.039575 -0.706771 0.7215552013-01-05 -1.087401 0.276232 0.567020 -0.4249722013-01-06 0.524988 -1.478427 0.113648 -0.673690 选择器 注释: 标准Python / Numpy表达式可以完成这些互动工作, 但在生产代码中, 我们推荐使用优化的pandas数据访问方法, .at, .iat, .loc, .iloc 和 .ix. 读取选择单列, 这会产生一个序列, 等价df.A 123456789In [23]: df['A']Out[23]: 2013-01-01 0.4691122013-01-02 1.2121122013-01-03 -0.8618492013-01-04 0.7215552013-01-05 -0.4249722013-01-06 -0.673690Freq: D, Name: A, dtype: float64 使用[]选择行片断 12345678910111213In [24]: df[0:3]Out[24]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.071804In [25]: df['20130102':'20130104']Out[25]: A B C D2013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.271860 使用[]选择行片断 12345678910111213In [24]: df[0:3]Out[24]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.071804In [25]: df['20130102':'20130104']Out[25]: A B C D2013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.271860 使用标签选择使用标签获取横截面 1234567In [26]: df.loc[dates[0]]Out[26]: A 0.469112B -0.282863C -1.509059D -1.135632Name: 2013-01-01 00:00:00, dtype: float64 使用标签选择多轴 123456789In [27]: df.loc[:,['A','B']]Out[27]: A B2013-01-01 0.469112 -0.2828632013-01-02 1.212112 -0.1732152013-01-03 -0.861849 -2.1045692013-01-04 0.721555 -0.7067712013-01-05 -0.424972 0.5670202013-01-06 -0.673690 0.113648 显示标签切片, 包含两个端点 123456In [28]: df.loc['20130102':'20130104',['A','B']]Out[28]: A B2013-01-02 1.212112 -0.1732152013-01-03 -0.861849 -2.1045692013-01-04 0.721555 -0.706771 降低返回对象维度 12345In [29]: df.loc['20130102',['A','B']]Out[29]: A 1.212112B -0.173215Name: 2013-01-02 00:00:00, dtype: float64 获取标量值 12In [30]: df.loc[dates[0],'A']Out[30]: 0.46911229990718628 快速访问并获取标量数据 (等价上面的方法) 12In [31]: df.at[dates[0],'A']Out[31]: 0.46911229990718628 按位置选择传递整数选择位置 1234567In [32]: df.iloc[3]Out[32]: A 0.721555B -0.706771C -1.039575D 0.271860Name: 2013-01-04 00:00:00, dtype: float64 使用整数片断,效果类似numpy/python 12345In [33]: df.iloc[3:5,0:2]Out[33]: A B2013-01-04 0.721555 -0.7067712013-01-05 -0.424972 0.567020 使用整数偏移定位列表,效果类似 numpy/python 样式 123456In [34]: df.iloc[[1,2,4],[0,2]]Out[34]: A C2013-01-02 1.212112 0.1192092013-01-03 -0.861849 -0.4949292013-01-05 -0.424972 0.276232 显式行切片 12345In [35]: df.iloc[1:3,:]Out[35]: A B C D2013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.071804 显式列切片 123456789In [36]: df.iloc[:,1:3]Out[36]: B C2013-01-01 -0.282863 -1.5090592013-01-02 -0.173215 0.1192092013-01-03 -2.104569 -0.4949292013-01-04 -0.706771 -1.0395752013-01-05 0.567020 0.2762322013-01-06 0.113648 -1.478427 显式获取一个值 12In [37]: df.iloc[1,1]Out[37]: -0.17321464905330861 快速访问一个标量（等同上个方法） 12In [38]: df.iat[1,1]Out[38]: -0.17321464905330861 布尔索引使用单个列的值选择数据 123456In [39]: df[df.A &gt; 0]Out[39]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-04 0.721555 -0.706771 -1.039575 0.271860 where 操作. 123456789In [40]: df[df &gt; 0]Out[40]: A B C D2013-01-01 0.469112 NaN NaN NaN2013-01-02 1.212112 NaN 0.119209 NaN2013-01-03 NaN NaN NaN 1.0718042013-01-04 0.721555 NaN NaN 0.2718602013-01-05 NaN 0.567020 0.276232 NaN2013-01-06 NaN 0.113648 NaN 0.524988 使用 isin() 筛选： 123456789101112131415161718In [41]: df2 = df.copy()In [42]: df2['E']=['one', 'one','two','three','four','three'] In [43]: df2Out[43]: A B C D E2013-01-01 0.469112 -0.282863 -1.509059 -1.135632 one2013-01-02 1.212112 -0.173215 0.119209 -1.044236 one2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 two2013-01-04 0.721555 -0.706771 -1.039575 0.271860 three2013-01-05 -0.424972 0.567020 0.276232 -1.087401 four2013-01-06 -0.673690 0.113648 -1.478427 0.524988 three In [44]: df2[df2['E'].isin(['two','four'])]Out[44]: A B C D E2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 two2013-01-05 -0.424972 0.567020 0.276232 -1.087401 four 赋值赋值一个新列，通过索引自动对齐数据 123456789101112In [45]: s1 = pd.Series([1,2,3,4,5,6],index=pd.date_range('20130102',periods=6))In [46]: s1Out[46]: 2013-01-02 12013-01-03 22013-01-04 32013-01-05 42013-01-06 52013-01-07 6Freq: D, dtype: int64 In [47]: df['F'] = s1 按标签赋值 1In [48]: df.at[dates[0],'A'] = 0 按位置赋值 12 In [49]: df.iat[0,1] = 0 通过numpy数组分配赋值 1In [50]: df.loc[:,'D'] = np.array([5] * len(df)) where 操作赋值. 1234567891011In [52]: df2 = df.copy()In [53]: df2[df2 &gt; 0] = -df2In [54]: df2Out[54]: A B C D F2013-01-01 0.000000 0.000000 -1.509059 -5 NaN2013-01-02 -1.212112 -0.173215 -0.119209 -5 -12013-01-03 -0.861849 -2.104569 -0.494929 -5 -22013-01-04 -0.721555 -0.706771 -1.039575 -5 -32013-01-05 -0.424972 -0.567020 -0.276232 -5 -42013-01-06 -0.673690 -0.113648 -1.478427 -5 -5 丢失的数据pandas主要使用np.nan替换丢失的数据. 默认情况下它并不包含在计算中. 请参阅 Missing Data section 重建索引允许更改/添加/删除指定轴索引,并返回数据副本. 123456789In [55]: df1 = df.reindex(index=dates[0:4],columns=list(df.columns) + ['E'])In [56]: df1.loc[dates[0]:dates[1],'E'] = 1In [57]: df1Out[57]: A B C D F E2013-01-01 0.000000 0.000000 -1.509059 5 NaN 12013-01-02 1.212112 -0.173215 0.119209 5 1 12013-01-03 -0.861849 -2.104569 -0.494929 5 2 NaN2013-01-04 0.721555 -0.706771 -1.039575 5 3 NaN 删除任何有丢失数据的行. 1234In [58]: df1.dropna(how='any')Out[58]: A B C D F E2013-01-02 1.212112 -0.173215 0.119209 5 1 1 填充丢失数据 1234567In [59]: df1.fillna(value=5)Out[59]: A B C D F E2013-01-01 0.000000 0.000000 -1.509059 5 5 12013-01-02 1.212112 -0.173215 0.119209 5 1 12013-01-03 -0.861849 -2.104569 -0.494929 5 2 52013-01-04 0.721555 -0.706771 -1.039575 5 3 5 获取值是否nan的布尔标记1234567In [60]: pd.isnull(df1)Out[60]: A B C D F E2013-01-01 False False False False True False2013-01-02 False False False False False False2013-01-03 False False False False False True2013-01-04 False False False False False True 运算统计计算时一般不包括丢失的数据 执行描述性统计12345678In [61]: df.mean()Out[61]: A -0.004474B -0.383981C -0.687758D 5.000000F 3.000000dtype: float64 在其他轴做相同的运算 123456789In [62]: df.mean(1)Out[62]: 2013-01-01 0.8727352013-01-02 1.4316212013-01-03 0.7077312013-01-04 1.3950422013-01-05 1.8836562013-01-06 1.592306Freq: D, dtype: float64 用于运算的对象有不同的维度并需要对齐.除此之外，pandas会自动沿着指定维度计算. 1234567891011121314151617181920In [63]: s = pd.Series([1,3,5,np.nan,6,8],index=dates).shift(2)In [64]: sOut[64]: 2013-01-01 NaN2013-01-02 NaN2013-01-03 12013-01-04 32013-01-05 52013-01-06 NaNFreq: D, dtype: float64 In [65]: df.sub(s,axis='index')Out[65]: A B C D F2013-01-01 NaN NaN NaN NaN NaN2013-01-02 NaN NaN NaN NaN NaN2013-01-03 -1.861849 -3.104569 -1.494929 4 12013-01-04 -2.278445 -3.706771 -4.039575 2 02013-01-05 -5.424972 -4.432980 -4.723768 0 -12013-01-06 NaN NaN NaN NaN NaN Apoly在数据上使用该函数 123456789101112131415161718In [66]: df.apply(np.cumsum)Out[66]: A B C D F2013-01-01 0.000000 0.000000 -1.509059 5 NaN2013-01-02 1.212112 -0.173215 -1.389850 10 12013-01-03 0.350263 -2.277784 -1.884779 15 32013-01-04 1.071818 -2.984555 -2.924354 20 62013-01-05 0.646846 -2.417535 -2.648122 25 102013-01-06 -0.026844 -2.303886 -4.126549 30 15 In [67]: df.apply(lambda x: x.max() - x.min())Out[67]: A 2.073961B 2.671590C 1.785291D 0.000000F 4.000000dtype: float64 直方图12345678910111213141516171819202122In [68]: s = pd.Series(np.random.randint(0,7,size=10))In [69]: sOut[69]: 0 41 22 13 24 65 46 47 68 49 4dtype: int32 In [70]: s.value_counts()Out[70]: 4 56 22 21 1dtype: int64 字符串方法序列可以使用一些字符串处理方法很轻易操作数据组中的每个元素,比如以下代码片断。 注意字符匹配方法默认情况下通常使用正则表达式（并且大多数时候都如此）. 更多信息请参阅字符串向量方法. 12345678910111213In [71]: s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'])In [72]: s.str.lower()Out[72]: 0 a1 b2 c3 aaba4 baca5 NaN6 caba7 dog8 catdtype: object 合并连接 pandas提供各种工具以简便合并序列,数据桢,和组合对象, 在连接/合并类型操作中使用多种类型索引和相关数学函数.把pandas对象连接到一起 123456789101112131415161718192021222324252627282930In [73]: df = pd.DataFrame(np.random.randn(10, 4))In [74]: dfOut[74]: 0 1 2 30 -0.548702 1.467327 -1.015962 -0.4830751 1.637550 -1.217659 -0.291519 -1.7455052 -0.263952 0.991460 -0.919069 0.2660463 -0.709661 1.669052 1.037882 -1.7057754 -0.919854 -0.042379 1.247642 -0.0099205 0.290213 0.495767 0.362949 1.5481066 -1.131345 -0.089329 0.337863 -0.9458677 -0.932132 1.956030 0.017587 -0.0166928 -0.575247 0.254161 -1.143704 0.2158979 1.193555 -0.077118 -0.408530 -0.862495 # break it into piecesIn [75]: pieces = [df[:3], df[3:7], df[7:]]In [76]: pd.concat(pieces)Out[76]: 0 1 2 30 -0.548702 1.467327 -1.015962 -0.4830751 1.637550 -1.217659 -0.291519 -1.7455052 -0.263952 0.991460 -0.919069 0.2660463 -0.709661 1.669052 1.037882 -1.7057754 -0.919854 -0.042379 1.247642 -0.0099205 0.290213 0.495767 0.362949 1.5481066 -1.131345 -0.089329 0.337863 -0.9458677 -0.932132 1.956030 0.017587 -0.0166928 -0.575247 0.254161 -1.143704 0.2158979 1.193555 -0.077118 -0.408530 -0.862495 连接123456789101112131415161718192021In [77]: left = pd.DataFrame(&#123;'key': ['foo', 'foo'], 'lval': [1, 2]&#125;)In [78]: right = pd.DataFrame(&#123;'key': ['foo', 'foo'], 'rval': [4, 5]&#125;)In [79]: leftOut[79]: key lval0 foo 11 foo 2 In [80]: rightOut[80]: key rval0 foo 41 foo 5 In [81]: pd.merge(left, right, on='key')Out[81]: key lval rval0 foo 1 41 foo 1 52 foo 2 43 foo 2 5 添加1234567891011121314151617181920212223242526In [82]: df = pd.DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])In [83]: dfOut[83]: A B C D0 1.346061 1.511763 1.627081 -0.9905821 -0.441652 1.211526 0.268520 0.0245802 -1.577585 0.396823 -0.105381 -0.5325323 1.453749 1.208843 -0.080952 -0.2646104 -0.727965 -0.589346 0.339969 -0.6932055 -0.339355 0.593616 0.884345 1.5914316 0.141809 0.220390 0.435589 0.1924517 -0.096701 0.803351 1.715071 -0.708758 In [84]: s = df.iloc[3]In [85]: df.append(s, ignore_index=True)Out[85]: A B C D0 1.346061 1.511763 1.627081 -0.9905821 -0.441652 1.211526 0.268520 0.0245802 -1.577585 0.396823 -0.105381 -0.5325323 1.453749 1.208843 -0.080952 -0.2646104 -0.727965 -0.589346 0.339969 -0.6932055 -0.339355 0.593616 0.884345 1.5914316 0.141809 0.220390 0.435589 0.1924517 -0.096701 0.803351 1.715071 -0.7087588 1.453749 1.208843 -0.080952 -0.264610 ## 分组对于“group by”指的是以下一个或多个处理 将数据按某些标准分割为不同的组 在每个独立组上应用函数 组合结果为一个数据结构 123456789101112131415161718In [86]: df = pd.DataFrame(&#123;'A' : ['foo', 'bar', 'foo', 'bar', ....: 'foo', 'bar', 'foo', 'foo'], ....: 'B' : ['one', 'one', 'two', 'three', ....: 'two', 'two', 'one', 'three'], ....: 'C' : np.random.randn(8), ....: 'D' : np.random.randn(8)&#125;) ....: In [87]: dfOut[87]: A B C D0 foo one -1.202872 -0.0552241 bar one -1.814470 2.3959852 foo two 1.018601 1.5528253 bar three -0.595447 0.1665994 foo two 1.395433 0.0476095 bar two -0.392670 -0.1364736 foo one 0.007207 -0.5617577 foo three 1.928123 -1.623033 分组然后应用函数统计总和存放到结果组 123456In [88]: df.groupby('A').sum()Out[88]: C DA bar -2.802588 2.42611foo 3.146492 -0.63958 按多列分组为层次索引,然后应用函数 12345678910In [89]: df.groupby(['A','B']).sum()Out[89]: C DA B bar one -1.814470 2.395985 three -0.595447 0.166599 two -0.392670 -0.136473foo one -1.195665 -0.616981 three 1.928123 -1.623033 two 2.414034 1.600434 重塑请参阅章节 分层索引 和 重塑.堆叠 1234567891011121314151617181920212223242526In [90]: tuples = list(zip(*[['bar', 'bar', 'baz', 'baz', ....: 'foo', 'foo', 'qux', 'qux'], ....: ['one', 'two', 'one', 'two', ....: 'one', 'two', 'one', 'two']])) ....: In [91]: tuplesOut[91]: [('bar', 'one'), ('bar', 'two'), ('baz', 'one'), ('baz', 'two'), ('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')]In [92]: index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])In [93]: df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=['A', 'B'])In [94]: df2 = df[:4]In [95]: df2Out[96]: A Bfirst second bar one 0.029399 -0.542108 two 0.282696 -0.087302baz one -1.575170 1.771208 two 0.816482 1.100230 堆叠 函数 “压缩” 数据桢的列一个级别. 12345678910111213In [95]: stacked = df2.stack()In [96]: stackedOut[96]: first second bar one A 0.029399 B -0.542108 two A 0.282696 B -0.087302baz one A -1.575170 B 1.771208 two A 0.816482 B 1.100230dtype: float64 被“堆叠”数据桢或序列(有多个索引作为索引), 其堆叠的反向操作是未堆栈, 上面的数据默认反堆叠到上一级别: 1234567891011121314151617181920212223242526In [97]: stacked.unstack()Out[97]: A Bfirst second bar one 0.029399 -0.542108 two 0.282696 -0.087302baz one -1.575170 1.771208 two 0.816482 1.100230 In [98]: stacked.unstack(1)Out[98]: second one twofirst bar A 0.029399 0.282696 B -0.542108 -0.087302baz A -1.575170 0.816482 B 1.771208 1.100230 In [99]: stacked.unstack(0)Out[99]: first bar bazsecond one A 0.029399 -1.575170 B -0.542108 1.771208two A 0.282696 0.816482 B -0.087302 1.100230 ### 数据透视表数据透视表 123456789101112131415161718192021In [100]: df = pd.DataFrame(&#123;'A' : ['one', 'one', 'two', 'three'] * 3, .....: 'B' : ['A', 'B', 'C'] * 4, .....: 'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 2, .....: 'D' : np.random.randn(12), .....: 'E' : np.random.randn(12)&#125;) .....: In [101]: dfOut[101]: A B C D E0 one A foo 1.418757 -0.1796661 one B foo -1.879024 1.2918362 two C foo 0.536826 -0.0096143 three A bar 1.006160 0.3921494 one B bar -0.029716 0.2645995 one C bar -1.146178 -0.0574096 two A foo 0.100900 -1.4256387 three B foo -1.035018 1.0240988 one C foo 0.314665 -0.1060629 one A bar -0.773723 1.82437510 two B bar -1.170653 0.59597411 three C bar 0.648740 1.167115 我们可以从此数据非常容易的产生数据透视表: 12345678910111213In [102]: pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'])Out[102]: C bar fooA B one A -0.773723 1.418757 B -0.029716 -1.879024 C -1.146178 0.314665three A 1.006160 NaN B NaN -1.035018 C 0.648740 NaNtwo A NaN 0.100900 B -1.170653 NaN C NaN 0.536826 时间序列pandas有易用,强大且高效的函数用于高频数据重采样转换操作(例如,转换秒数据到5分钟数据), 这是很普遍的情况，但并不局限于金融应用, 请参阅时间序列章节 123456In [103]: rng = pd.date_range('1/1/2012', periods=100, freq='S')In [104]: ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)In [105]: ts.resample('5Min', how='sum')Out[105]: 2012-01-01 25083Freq: 5T, dtype: int32 5Min是按5min为一个阶段进行group 时区表示 1234567891011121314151617181920In [106]: rng = pd.date_range('3/6/2012 00:00', periods=5, freq='D')In [107]: ts = pd.Series(np.random.randn(len(rng)), rng)In [108]: tsOut[108]: 2012-03-06 0.4640002012-03-07 0.2273712012-03-08 -0.4969222012-03-09 0.3063892012-03-10 -2.290613Freq: D, dtype: float64 In [109]: ts_utc = ts.tz_localize('UTC')In [110]: ts_utcOut[110]: 2012-03-06 00:00:00+00:00 0.4640002012-03-07 00:00:00+00:00 0.2273712012-03-08 00:00:00+00:00 -0.4969222012-03-09 00:00:00+00:00 0.3063892012-03-10 00:00:00+00:00 -2.290613Freq: D, dtype: float64 转换到其它时区 12345678In [111]: ts_utc.tz_convert('US/Eastern')Out[111]: 2012-03-05 19:00:00-05:00 0.4640002012-03-06 19:00:00-05:00 0.2273712012-03-07 19:00:00-05:00 -0.4969222012-03-08 19:00:00-05:00 0.3063892012-03-09 19:00:00-05:00 -2.290613Freq: D, dtype: float64 转换不同的时间跨度 1234567891011121314151617181920212223242526272829In [112]: rng = pd.date_range('1/1/2012', periods=5, freq='M')In [113]: ts = pd.Series(np.random.randn(len(rng)), index=rng)In [114]: tsOut[114]: 2012-01-31 -1.1346232012-02-29 -1.5618192012-03-31 -0.2608382012-04-30 0.2819572012-05-31 1.523962Freq: M, dtype: float64 In [115]: ps = ts.to_period()In [116]: psOut[116]: 2012-01 -1.1346232012-02 -1.5618192012-03 -0.2608382012-04 0.2819572012-05 1.523962Freq: M, dtype: float64 In [117]: ps.to_timestamp()Out[117]: 2012-01-01 -1.1346232012-02-01 -1.5618192012-03-01 -0.2608382012-04-01 0.2819572012-05-01 1.523962Freq: MS, dtype: float64 转换时段并且使用一些运算函数, 下例中, 我们转换年报11月到季度结束每日上午9点数据 1234567891011In [118]: prng = pd.period_range('1990Q1', '2000Q4', freq='Q-NOV')In [119]: ts = pd.Series(np.random.randn(len(prng)), prng)In [120]: ts.index = (prng.asfreq('M', 'e') + 1).asfreq('H', 's') + 9In [121]: ts.head()Out[121]: 1990-03-01 09:00 -0.9029371990-06-01 09:00 0.0681591990-09-01 09:00 -0.0578731990-12-01 09:00 -0.3682041991-03-01 09:00 -1.144073Freq: H, dtype: float64 分类自版本0.15起, pandas可以在数据桢中包含分类. 完整的文档, 请查看分类介绍 and the API文档. 1In [122]: df = pd.DataFrame(&#123;"id":[1,2,3,4,5,6], "raw_grade":['a', 'b', 'b', 'a', 'a', 'e']&#125;) 转换原始类别为分类数据类型.1234567891011In [123]: df["grade"] = df["raw_grade"].astype("category")In [124]: df["grade"]Out[124]: 0 a1 b2 b3 a4 a5 eName: grade, dtype: categoryCategories (3, object): [a, b, e] 重命令分类为更有意义的名称 (分配到Series.cat.categories对应位置!)1df["grade"].cat.categories = ["very good", "good", "very bad"] 重排顺分类,同时添加缺少的分类(序列 .cat方法下返回新默认序列) 1234567891011In [126]: df["grade"] = df["grade"].cat.set_categories(["very bad", "bad", "medium", "good", "very good"])In [127]: df["grade"]Out[127]: 0 very good1 good2 good3 very good4 very good5 very badName: grade, dtype: categoryCategories (5, object): [very bad, bad, medium, good, very good] 排列分类中的顺序,不是按词汇排列. 123456789In [128]: df.sort("grade")Out[128]: id raw_grade grade5 6 e very bad1 2 b good2 3 b good0 1 a very good3 4 a very good4 5 a very good 类别列分组,并且也显示空类别. 123456789In [129]: df.groupby("grade").size()Out[129]: gradevery bad 1bad NaNmedium NaNgood 2very good 3dtype: float64 绘图1234In [130]: ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))In [131]: ts = ts.cumsum()In [132]: ts.plot()Out[132]: &lt;matplotlib.axes._subplots.AxesSubplot at 0xb02091ac&gt; 在数据桢中,可以很方便的绘制带标签列: 123456In [133]: df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, .....: columns=['A', 'B', 'C', 'D']) .....: In [134]: df = df.cumsum()In [135]: plt.figure(); df.plot(); plt.legend(loc='best')Out[135]: &lt;matplotlib.legend.Legend at 0xb01c9cac&gt; 获取数据输入/输出CSV写入csv文件1In [136]: df.to_csv('foo.csv') 读取csv文件 1234567891011121314151617181920In [137]: pd.read_csv('foo.csv')Out[137]: Unnamed: 0 A B C D0 2000-01-01 0.266457 -0.399641 -0.219582 1.1868601 2000-01-02 -1.170732 -0.345873 1.653061 -0.2829532 2000-01-03 -1.734933 0.530468 2.060811 -0.5155363 2000-01-04 -1.555121 1.452620 0.239859 -1.1568964 2000-01-05 0.578117 0.511371 0.103552 -2.4282025 2000-01-06 0.478344 0.449933 -0.741620 -1.9624096 2000-01-07 1.235339 -0.091757 -1.543861 -1.084753.. ... ... ... ... ...993 2002-09-20 -10.628548 -9.153563 -7.883146 28.313940994 2002-09-21 -10.390377 -8.727491 -6.399645 30.914107995 2002-09-22 -8.985362 -8.485624 -4.669462 31.367740996 2002-09-23 -9.558560 -8.781216 -4.499815 30.518439997 2002-09-24 -9.902058 -9.340490 -4.386639 30.105593998 2002-09-25 -10.216020 -9.480682 -3.933802 29.758560999 2002-09-26 -11.856774 -10.671012 -3.216025 29.369368 [1000 rows x 5 columns] HDF5读写HDF存储 写入HDF5存储 1In [138]: df.to_hdf('foo.h5','df') 读取HDF5存储 1234567891011121314151617181920In [139]: pd.read_hdf('foo.h5','df')Out[139]: A B C D2000-01-01 0.266457 -0.399641 -0.219582 1.1868602000-01-02 -1.170732 -0.345873 1.653061 -0.2829532000-01-03 -1.734933 0.530468 2.060811 -0.5155362000-01-04 -1.555121 1.452620 0.239859 -1.1568962000-01-05 0.578117 0.511371 0.103552 -2.4282022000-01-06 0.478344 0.449933 -0.741620 -1.9624092000-01-07 1.235339 -0.091757 -1.543861 -1.084753... ... ... ... ...2002-09-20 -10.628548 -9.153563 -7.883146 28.3139402002-09-21 -10.390377 -8.727491 -6.399645 30.9141072002-09-22 -8.985362 -8.485624 -4.669462 31.3677402002-09-23 -9.558560 -8.781216 -4.499815 30.5184392002-09-24 -9.902058 -9.340490 -4.386639 30.1055932002-09-25 -10.216020 -9.480682 -3.933802 29.7585602002-09-26 -11.856774 -10.671012 -3.216025 29.369368 [1000 rows x 4 columns] Excel读写MS Excel 写入excel文件 1In [140]: df.to_excel('foo.xlsx', sheet_name='Sheet1') 读取excel文件 1234567891011121314151617181920In [141]: pd.read_excel('foo.xlsx', 'Sheet1', index_col=None, na_values=['NA'])Out[141]: A B C D2000-01-01 0.266457 -0.399641 -0.219582 1.1868602000-01-02 -1.170732 -0.345873 1.653061 -0.2829532000-01-03 -1.734933 0.530468 2.060811 -0.5155362000-01-04 -1.555121 1.452620 0.239859 -1.1568962000-01-05 0.578117 0.511371 0.103552 -2.4282022000-01-06 0.478344 0.449933 -0.741620 -1.9624092000-01-07 1.235339 -0.091757 -1.543861 -1.084753... ... ... ... ...2002-09-20 -10.628548 -9.153563 -7.883146 28.3139402002-09-21 -10.390377 -8.727491 -6.399645 30.9141072002-09-22 -8.985362 -8.485624 -4.669462 31.3677402002-09-23 -9.558560 -8.781216 -4.499815 30.5184392002-09-24 -9.902058 -9.340490 -4.386639 30.1055932002-09-25 -10.216020 -9.480682 -3.933802 29.7585602002-09-26 -11.856774 -10.671012 -3.216025 29.369368 [1000 rows x 4 columns] 陷阱如果尝试这样操作可能会看到像这样的异常: 12345&gt;&gt;&gt; if pd.Series([False, True, False]): print("I was true")Traceback ...ValueError: The truth value of an array is ambiguous. Use a.empty, a.any() or a.all(). 查看对照获取解释和怎么做的帮助 也可以查看陷阱.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Number of permutation with K inversions]]></title>
      <url>%2F2017%2F05%2F26%2FNumber-of-permutation-with-K-inversions%2F</url>
      <content type="text"><![CDATA[Given an array, an inversion is defined as a pair a[i], a[j] such that a[i] &gt; a[j] and i &lt; j. We are given two numbers N and k, we need to tell how many permutation of first N number have exactly K inversion. 给定一个数组，inversion被定义为a [i]，a [j]，使得[i]&gt; a [j]和i &lt; j。 我们给出两个数字N和k，我们需要计算出 N 个数的排列有多少个正好是K个inversion。 例如:输入 : N = 3, K = 1输出 : 2Explanation :全部的可能情况：123, 132, 213, 231, 312, 321符合条件的情况 : 132 and 213 输入 : N = 4, K = 2 输出 : 2 解法的伪代码如下：1234567891011If N is 0, Count(0, K) = 0If K is 0, Count(N, 0) = 1 (Only sorted array)In general case, If we have N number and require K inversion, Count(N, K) = Count(N - 1, K) + Count(N – 1, K - 1) + Count(N – 1, K – 2) + .... + Count(N – 1, 0) 其解法思想如下： 如果我们有N个数并希望有K个inversion，并且假设（N-1）个数的所有inversion写在某个地方，那么新数（第N个数，即最大的数）需要被置于（N-1）个数的所有inversion中，在我们的答案中应该加上那些inversion计数变为K后的组合。 简单的说就是将第N个数插入在前N-1个数的所有组合中，将inversion=k的情况相加。下面给出了各个情况的示意图： - 前n-1个已经组成了inversions=k，则第n个只需要插入在最后。 - 前n-1个已经组成了inversions=k-1，则第n个需要往前插 1 位，让inversions=k。 - 前n-1个已经组成了inversions=k-2，则第n个需要往前插 2 位，让inversions=k。 - 依次类推： 实现代码如下（c++）：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// C++ program to find number of permutation with// K inversion using Memoization#include &lt;bits/stdc++.h&gt;using namespace std; // Limit on N and Kconst int M = 100 // 2D array memo for stopping solving same problem// againint memo[M][M]; // method recursively calculates permutation with// K inversionint numberOfPermWithKInversion(int N, int K)&#123; // base cases if (N == 0) return 0; if (K == 0) return 1; // if already solved then return result directly if (memo[N][K] != 0) return memo[N][K]; // calling recursively all subproblem of // permutation size N - 1 int sum = 0; for (int i = 0; i &lt;= K; i++) &#123; // Call recursively only if total inversion // to be made are less than size if (i &lt;= N - 1) sum += numberOfPermWithKInversion(N-1, K-i); &#125; // store result into memo memo[N][K] = sum; return sum;&#125; // Driver code to test above methodsint main()&#123; int N = 4; int K = 2; cout &lt;&lt; numberOfPermWithKInversion(N, K); return 0;&#125; Output: 5]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[编程刷题-1]]></title>
      <url>%2F2017%2F05%2F25%2F%E7%BC%96%E7%A8%8B%E5%88%B7%E9%A2%98-1%2F</url>
      <content type="text"><![CDATA[一种双核CPU的两个核能够同时的处理任务，现在有n个已知数据量的任务需要交给CPU处理，假设已知CPU的每个核1秒可以处理1kb，每个核同时只能处理一项任务。n个任务可以按照任意顺序放入CPU进行处理，现在需要设计一个方案让CPU处理完这批任务所需的时间最少，求这个最小的时间。 输入描述: 输入包括两行：第一行为整数n(1 ≤ n ≤ 50)第二行为n个整数lengthi ，表示每个任务的长度为length[i]kb，每个数均为1024的倍数。 输出描述: 输出一个整数，表示最少需要处理的时间 输入例子: 53072 3072 7168 3072 1024 输出例子: 9216 解法: 这里面你用到了动态规划的思想（01背包的问题），意思就是说：所有的数据的长度和价值是相等的，我有一个容量为所有数据长度一半的背包，让这个背包装的数据的价值尽可能的大。这样的话就能做到尽可能的将数据均分为两份（注意：不能做到完全均分，只能尽可能的保证），均分成两份后，就能使用长度较大的那部分计算处理时间。这里有一个小小的需要注意的地方，数据的长度很大，使用int和long都会出现不够用的情况，但是巧在都是1024的倍数，所以进行背包均分的时候，将数据的长度和价值都 除以1024。 下面给出我的代码： 123456789101112131415161718192021222324252627282930313233343536373839import java.util.Scanner;public class Main &#123; public static int ZeroOnePackage(int N,int V,int w[])&#123; int [] packages=new int[V+1]; for (int i=1;i&lt;=N;i++)&#123; for (int j=V;j&gt;=w[i];j--)&#123; packages[j]=Math.max(packages[j],packages[j-w[i]]+w[i]); &#125; &#125; return packages[V]; &#125; public static void main(String args[])&#123; Scanner scanner=new Scanner(System.in); int N=scanner.nextInt(); int w[]=new int[N+1]; int length_all=0; for (int i=1;i&lt;=N;i++)&#123; w[i]=scanner.nextInt()/1024; length_all+=w[i]; &#125; int length=length_all/2; int packages=ZeroOnePackage(N,length,w); long maxLenth; if(length_all&gt;packages*2)&#123; maxLenth=length_all-packages; &#125;else&#123; maxLenth=packages; &#125; System.out.println(maxLenth*1024); &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[观察者模式]]></title>
      <url>%2F2017%2F05%2F25%2F%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[定义：观察者模式定义了对象之间的一对多依赖，这样一来，当一个对象改变状态时，它的所有依赖都会收到通知并自动更新。 模式总结优点观察者模式解除了主题和具体观察者的耦合，让耦合的双方都依赖于抽象，而不是依赖具体。从而使得各自的变化都不会影响另一边的变化。 缺点依赖关系并未完全解除，抽象通知者依旧依赖抽象的观察者。 适用场景1.当一个对象的改变需要给变其它对象时，而且它不知道具体有多少个对象有待改变时。 2.一个抽象某型有两个方面，当其中一个方面依赖于另一个方面，这时用观察者模式可以将这两者封装在独立的对象中使它们各自独立地改变和复用。 要点 观察者模式定义了对象之间一对多的关系 主题（也就是可观察者）用一个共同的接口来更新观察者 观察者和可观察者之间用松耦合的方式结合，可观察者不知道观察者的细节，只知道观察者实现了观察者接口。 使用此模式时，你可以从被观察者处推或拉数据（然而，推的方式被认为更正确）。 有多个观察者是，不可以依赖特定的通知顺序。 Swing大量使用观察者模式，许多GUI框架也是如此。 此模式也被应用于许多地方，如：JavaBeans、RMI。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[工厂模式]]></title>
      <url>%2F2017%2F05%2F25%2F%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[工厂模式所有的工厂模式都用来封装对象的创建。工厂方法模式通过让子类决定该创建的对象是什么，来达到将对象创建的过程封装的目的。正式定义：工厂方法模式定义了一个创建对象的接口，但由子类决定要实例化的类是哪一个。工厂方法让类把实例化推迟到子类。这里所谓的决定并不是指模式允许子类本身在运行时做决定，而是指在编写创建者类时，不需要知道实际创建的产品是哪一个。 抽象工厂模式 定义：抽象工厂模式提供一个接口，用于创建相关或依赖对象的家族，而不需要明确指定具体类。 工厂方法模式 抽象工厂模式 针对的是一个产品等级结构 针对的是面向多个产品等级结构 一个抽象产品类 多个抽象产品类 可以派生出多个具体产品类 每个抽象产品类可以派生出多个具体产品类 一个抽象工厂类，可以派生出多个具体工厂类 一个抽象工厂类，可以派生出多个具体工厂类 每个具体工厂类只能创建一个具体产品类的实例 每个具体工厂类可以创建多个具体产品类的实例]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[装饰者模式]]></title>
      <url>%2F2017%2F05%2F25%2F%E8%A3%85%E9%A5%B0%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[定义：装饰者模式，动态地将责任附加到对象上。若要扩展功能，装饰者提供了比继承更加有弹性的替代方案。 优点1、装饰者模式可以提供比继承更多的灵活性。2、可以通过一种动态的方式来扩展一个对象的功能，在运行时选择不同的装饰器，从而实现不同的行为。3、通过使用不同的具体装饰类以及这些装饰类的排列组合，可以创造出很多不同行为的组合。可以使用多个具体装饰类来装饰同一对象，得到功能更为强大的对象。4、具体构件类与具体装饰类可以独立变化，用户可以根据需要增加新的具体构件类和具体装饰类，在使用时再对其进行组合，原有代码无须改变，符合“开闭原则”。 缺点1、会产生很多的小对象，增加了系统的复杂性2、这种比继承更加灵活机动的特性，也同时意味着装饰模式比继承更加易于出错，排错也很困难，对于多次装饰的对象，调试时寻找错误可能需要逐级排查，较为烦琐。 装饰者的使用场景1、在不影响其他对象的情况下，以动态、透明的方式给单个对象添加职责。2、需要动态地给一个对象增加功能，这些功能也可以动态地被撤销。当不能采用继承的方式对系统进行扩充或者采用继承不利于系统扩展和维护时。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[策略模式]]></title>
      <url>%2F2017%2F05%2F25%2F%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[策略模式策略模式的定义：策略模式定义了算法族，分别封装起来，让她们之间可以互相替换，此模式让算法的变化独立于使用算法的客户。 在使用策略模式时，多用组合，少用继承。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[用简单的方式讲解KMP算法]]></title>
      <url>%2F2017%2F05%2F25%2F%E7%94%A8%E7%AE%80%E5%8D%95%E7%9A%84%E6%96%B9%E5%BC%8F%E8%AE%B2%E8%A7%A3KMP%E7%AE%97%E6%B3%95%2F</url>
      <content type="text"><![CDATA[之前对kmp算法虽然了解它的原理，即求出P0···Pi的最大相同前后缀长度k；但是问题在于如何求出这个最大前后缀长度呢？我觉得网上很多帖子都说的不是很清楚，总感觉没有把那层纸戳破，后来翻看算法导论，32章 字符串匹配虽然讲到了对前后缀计算的正确性，但是大量的推理证明不大好理解，没有与程序结合起来讲。今天我在这里讲一讲我的一些理解，希望大家多多指教，如果有不清楚的或错误的请给我留言。 原理字符串匹配是计算机的基本任务之一。举例来说，有一个字符串”BBC ABCDAB ABCDABCDABDE”，我想知道，里面是否包含另一个字符串”ABCDABD”？ 许多算法可以完成这个任务，Knuth-Morris-Pratt算法（简称KMP）是最常用的之一。它以三个发明者命名，起头的那个K就是著名科学家Donald Knuth。 这种算法不太容易理解，网上有很多解释，但读起来都很费劲。直到读到Jake Boxer的文章，我才真正理解这种算法。下面，我用自己的语言，试图写一篇比较好懂的KMP算法解释。 首先，字符串”BBC ABCDAB ABCDABCDABDE”的第一个字符与搜索词”ABCDABD”的第一个字符，进行比较。因为B与A不匹配，所以搜索词后移一位。 因为B与A不匹配，搜索词再往后移。 就这样，直到字符串有一个字符，与搜索词的第一个字符相同为止。 接着比较字符串和搜索词的下一个字符，还是相同。 直到字符串有一个字符，与搜索词对应的字符不相同为止。 这时，最自然的反应是，将搜索词整个后移一位，再从头逐个比较。这样做虽然可行，但是效率很差，因为你要把”搜索位置”移到已经比较过的位置，重比一遍。 一个基本事实是，当空格与D不匹配时，你其实知道前面六个字符是”ABCDAB”。KMP算法的想法是，设法利用这个已知信息，不要把”搜索位置”移回已经比较过的位置，继续把它向后移，这样就提高了效率。 怎么做到这一点呢？可以针对搜索词，算出一张《部分匹配表》（Partial Match Table）。这张表是如何产生的，后面再介绍，这里只要会用就可以了。 已知空格与D不匹配时，前面六个字符”ABCDAB”是匹配的。查表可知，最后一个匹配字符B对应的”部分匹配值”为2，因此按照下面的公式算出向后移动的位数： 移动位数 = 已匹配的字符数 - 对应的部分匹配值因为 6 - 2 等于4，所以将搜索词向后移动4位。 因为空格与Ｃ不匹配，搜索词还要继续往后移。这时，已匹配的字符数为2（”AB”），对应的”部分匹配值”为0。所以，移动位数 = 2 - 0，结果为 2，于是将搜索词向后移2位。 因为空格与A不匹配，继续后移一位。 逐位比较，直到发现C与D不匹配。于是，移动位数 = 6 - 2，继续将搜索词向后移动4位。 逐位比较，直到搜索词的最后一位，发现完全匹配，于是搜索完成。如果还要继续搜索（即找出全部匹配），移动位数 = 7 - 0，再将搜索词向后移动7位，这里就不再重复了。 下面介绍《部分匹配表》是如何产生的。首先，要了解两个概念：”前缀”和”后缀”。 “前缀”指除了最后一个字符以外，一个字符串的全部头部组合；”后缀”指除了第一个字符以外，一个字符串的全部尾部组合。 “部分匹配值”就是”前缀”和”后缀”的最长的共有元素的长度。以”ABCDABD”为例， － “A”的前缀和后缀都为空集，共有元素的长度为0； － “AB”的前缀为[A]，后缀为[B]，共有元素的长度为0； － “ABC”的前缀为[A, AB]，后缀为[BC, C]，共有元素的长度0； － “ABCD”的前缀为[A, AB, ABC]，后缀为[BCD, CD, D]，共有元素的长度为0； － “ABCDA”的前缀为[A, AB, ABC, ABCD]，后缀为[BCDA, CDA, DA, A]，共有元素为”A”，长度为1； － “ABCDAB”的前缀为[A, AB, ABC, ABCD, ABCDA]，后缀为[BCDAB, CDAB, DAB, AB, B]，共有元素为”AB”，长度为2； － “ABCDABD”的前缀为[A, AB, ABC, ABCD, ABCDA, ABCDAB]，后缀为[BCDABD, CDABD, DABD, ABD, BD, D]，共有元素的长度为0。 “部分匹配”的实质是，有时候，字符串头部和尾部会有重复。比如，”ABCDAB”之中有两个”AB”，那么它的”部分匹配值”就是2（”AB”的长度）。搜索词移动的时候，第一个”AB”向后移动4位（字符串长度-部分匹配值），就可以来到第二个”AB”的位置. http://www.cnblogs.com/c-cloud/p/3224788.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[多重背包]]></title>
      <url>%2F2017%2F05%2F22%2F%E5%A4%9A%E9%87%8D%E8%83%8C%E5%8C%85%2F</url>
      <content type="text"><![CDATA[题目：有N种物品和一个容量为V的背包。第i种物品最多有n[i]件可用，每件费用是c[i]，价值是w[i]。求解将哪些物品装入背包可使这些物品的费用总和不超过背包容量，且价值总和最大。 基本算法这题目和完全背包问题很类似。基本的方程只需将完全背包问题的方程略微一改即可，因为对于第i种物品有n[i]+1种策略：取0件，取1件……取n[i]件。令f[i][v]表示前i种物品恰放入一个容量为v的背包的最大权值，则有状态转移方程： f[i][v]=max{f[i-1][v-kc[i]]+kw[i]|0&lt;=k&lt;=n[i]} 复杂度是O(V*Σn[i])。 相应的代码实现为： 12345678910111213public static int MultiPackage(int n,int v,int []w,int []c,int [] num)&#123; int [][] f=new int[n+1][v+1]; for (int i=1;i&lt;=n;i++)&#123; f[i][0]=0; for (int j=w[i];j&lt;=v;j++)&#123; int ncount=Math.min(num[i],j/w[i]); for (int k=0;k&lt;=ncount;k++)&#123; f[i][j]=Math.max(f[i][j],f[i-1][j-k*w[i]]+k*c[i]); &#125; &#125; &#125; return f[n][v]; &#125; 转化为0-1 背包的问题另一种好想好写的基本方法是转化为0-1背包求解：把第i种物品换成n[i]件01背包中的物品，则得到了物品数为Σn[i]的01背包问题，直接求解，复杂度仍然是O(V*Σn[i])。简单的说就是把所有的物品全部展开放在一个数组中，当做0-1背包来解决。 1234567891011121314151617181920212223242526272829303132//o1背包求解public static int ZeroOnePackage(int n,int v,int [] w,int [] c)&#123; int [] f=new int[v+1]; for (int i=1;i&lt;=n;i++)&#123; for (int j=v;j&gt;=w[i];j--)&#123; f[j]=Math.max(f[j],f[j-w[i]]+c[i]); &#125; &#125; return f[v];&#125; //多重背包求解public static int MultiPackage_2(int n,int v,int []w,int []c,int [] num)&#123; int length=0; for (int i=1;i&lt;=n;i++) &#123; for (int j=1;j&lt;=num[i];j++)&#123; length++; &#125; &#125; int[] w_1=new int[length+1]; int[] c_1=new int[length+1]; int index=1; for (int i=1;i&lt;=n;i++) &#123; for (int j=1;j&lt;=num[i];j++)&#123; w_1[index]=w[i]; c_1[index++]=c[i]; &#125; &#125; return ZeroOnePackage(length,v,w_1,c_1);&#125; 但是我们期望将它转化为01背包问题之后能够像完全背包一样降低复杂度(O(VN))。仍然考虑二进制的思想，我们考虑把第i种物品换成若干件物品，使得原问题中第i种物品可取的每种策略——取0..n[i]件——均能等价于取若干件代换以后的物品。另外，取超过n[i]件的策略必不能出现。 方法是：将第i种物品分成若干件物品，其中每件物品有一个系数，这件物品的费用和价值均是原来的费用和价值乘以这个系数。使这些系数分别为1,2,4,…,2^(k-1), n[i]-2^k +1，且k是满足n[i]-2^k+1&gt;0的最大整数。例如，如果n[i]为13，就将这种物品分成系数分别为1,2,4,6的四件物品。 分成的这几件物品的系数和为n[i]，表明不可能取多于n[i]件的第i种物品。另外这种方法也能保证对于0..n[i]间的每一个整数，均可以用若干个系数的和表示，这个证明可以分0..2^k -1和2^k..n[i]两段来分别讨论得出，并不难，希望你自己思考尝试一下。 这样就将第i种物品分成了O(log n[i])种物品，将原问题转化为了复杂度为O(V*Σlog n[i])的01背包问题，是很大的改进。 O(VN)的算法 多重背包问题同样有O(VN)的算法。这个算法基于基本算法的状态转移方程，但应用单调队列的方法使每个状态的值可以以均摊O(1)的时间求解。由于用单调队列优化的DP已超出了NOIP的范围，故本文不再展开讲解。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[0-1背包]]></title>
      <url>%2F2017%2F05%2F22%2F0-1%E8%83%8C%E5%8C%85%2F</url>
      <content type="text"><![CDATA[0-1背包问题有很多介绍，现在先不做说明，有时间补上。以下是递归的实现： 12345678910111213141516171819202122232425262728293031323334package test;public class Package01 &#123; public int f(int n, int V,int[] w,int[] v) &#123; if (n==0 || V==0)//当物品数量为0，或者背包容量为0时，最优解为0 &#123; return 0; &#125; else &#123; //如果当前要判断的物品重量大于背包当前所剩的容量，那么就不选择这个物品 //在这种情况的最优解为f(n-1,C) if (w[n-1]&gt;V) return f(n-1,V,w,v); else &#123; //如果当前待判断的物品重量wi int tmp1 = f(n-1,V,w,v);//不选择物品i的情况下的最优解 int tmp2 = v[n-1] + f(n-1,V-w[n-1],w,v);//选择物品i的情况下的最优解 //返回选择物品i和不选择物品i中最优解大的一个 return tmp1 &gt; tmp2?tmp1:tmp2; &#125; &#125; &#125; public static void main(String args[]) &#123; int w[]=&#123;1,3,4,5&#125;;//物品重量数组 int v[]=&#123;2,30,44,20&#125;;//物品价值数组 Package01 pa=new Package01(); int maxvalue = pa.f(4,4,w,v); System.out.println("Maximum Value is: "+maxvalue); &#125;&#125; 非递归的实现如下： 123456789public static int package(int n,int V,int [] w,int [] v)&#123; int [] f=new int[V+1]; for (int i=1;i&lt;=n;i++)&#123; for (int j=V;j&gt;=w[i];j--)&#123; f[j]=Math.max(f[j],f[j-w[i]]+v[i]); &#125; &#125; return f[V]; &#125; 非递归的过程如下：测试数据：10,33,44,55,6 先将f[] 全部初始化为0，其实也代表这个背包数组适应物品为空的时候所能容纳的最大物品价值。 下图中，黄色为当前正在操作的时候，黄色以上为上一次的数组状态，黄色以下可以不用看，那是以后的状态。 之后的思想是将物品不断添加进物品数组中，然后更新背包数组的所能容纳的最大物品价值。添加第一个物品后：。。。添加完毕时是最后一行。需要注意的是在更新背包所能容纳的最大物品价值时需要从后往前去更新。 因为按照背包问题的求解策略，大的背包容量所能容纳的最大物品价值是根据小 的背包容量所能容纳的最大物品价值计算出的，先更新大的背包容量的值不会影响之前的小的背包容量。 完全背包和0-1在非递归实现上相似，只是完全背包是使用正序的去更新数组，而0-1背包是逆序的去更新。 完全背包可以参考我的另一篇文章（自己的博客网站）:http://amazingwu.xyz/2017/05/22/%E5%AE%8C%E5%85%A8%E8%83%8C%E5%8C%85/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[完全背包]]></title>
      <url>%2F2017%2F05%2F22%2F%E5%AE%8C%E5%85%A8%E8%83%8C%E5%8C%85%2F</url>
      <content type="text"><![CDATA[讲解及代码实现&emsp;&emsp;完全背包是在N种物品中选取若干件（同一种物品可多次选取）放在空间为V的背包里，每种物品的体积为C1，C2，…，Cn，与之相对应的价值为W1,W2，…，Wn.求解怎么装物品可使背包里物品总价值最大。动态规划（DP）：&emsp;&emsp;1） 子问题定义：F[i][j]表示前i种物品中选取若干件物品放入剩余空间为j的背包中所能得到的最大价值。&emsp;&emsp;2） 根据第i种物品放多少件进行决策。&emsp;&emsp;其中F[i-1][j-KC[i]]+KW[i]表示前i-1种物品中选取若干件物品放入剩余空间为j-K*C[i]的背包中所能得到的最大价值加上k件第i种物品；&emsp;&emsp;设物品种数为N，背包容量为V，第i种物品体积为C[i]，第i种物品价值为W[i]。非递归实现如下： 123456789public static int fullPackage(int n,int v,int [] c,int [] w)&#123; int [] f=new int[v+1]; for (int i=1;i&lt;=n;i++)&#123; for (int j=c[i];j&lt;=v;j++)&#123; f[j]=Math.max(f[j],f[j-c[i]]+w[i]); &#125; &#125; return f[v];&#125; &emsp;&emsp;这里是正序去更新数组，逆序的话就变成了0-1背包。 完全背包例题： 题目描述设有n种物品，每种物品有一个重量及一个价值。但每种物品的数量是无限的，同时有一个背包，最大载重量为M，今从n种物品中选取若干件(同一种物品可以多次选取)，使其重量的和小于等于M，而价值的和为最大。 输入第一行：两个整数，M(背包容量，M&lt;=200)和N(物品数量，N&lt;=30)；第2..N+1行：每行二个整数Wi,Ci，表示每个物品的重量和价值。 输出仅一行，一个数，表示最大总价值。 样例输入10 42 13 34 57 9样例输出max=12]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hive的UDF、UDAF、UDTF]]></title>
      <url>%2F2017%2F05%2F20%2FHive%E7%9A%84UDF%E3%80%81UDAF%E3%80%81UDTF%2F</url>
      <content type="text"><![CDATA[本篇文章详细讲解UDF、UDAF、UDTF的使用及注意事项，下章将会讲解在UDF的基础之上使用Hive 2.1中transform接口。 Hive进行UDF开发十分简单，此处所说UDF为Temporary的function，所以需要hive版本在0.4.0以上才可以。 一、背景：Hive是基于Hadoop中的MapReduce，提供HQL查询的数据仓库。Hive是一个很开放的系统，很多内容都支持用户定制，包括：a）文件格式：Text File，Sequence Fileb）内存中的数据格式： Java Integer/String, Hadoop IntWritable/Textc）用户提供的 map/reduce 脚本：不管什么语言，利用 stdin/stdout 传输数据d）用户自定义函数: Substr, Trim, 1 – 1e）用户自定义聚合函数: Sum, Average…… n – 1 二、用法UDF定义：UDF(User-Defined-Function)，用户自定义函数对数据进行处理。UDF函数可以直接应用于select语句，对查询结构做格式化处理后，再输出内容。Hive的UDF根据继承的父类可以分为UDF类（指类别，旧）和GenericUDF类，新版本的Hive陆续把部分UDF类的UDF，改写成了GenericUDF类的UDF。实现自己的Hive的UDF，一种是直接继承Hive中的org.apache.hadoop.hive.ql.UDF，另一种是继承org.apache.hadoop.hive.ql.udf.generic.GenericUDF。1.第一种方法使用UDF函数的时候需要注意一下几点：a）自定义UDF需要继承org.apache.hadoop.hive.ql.UDF。b）需要实现evaluate函。c）evaluate函数支持重载。以下是两个数求和函数的UDF。evaluate函数代表两个整型数据相加，两个浮点型数据相加，可变长数据相加，Hive的UDF开发只需要重构UDF类的evaluate函数即可。例： 1234567891011121314151617181920import org.apache.hadoop.hive.ql.exec.UDF;public final class Add extends UDF &#123;public Integer evaluate(Integer a, Integer b) &#123; if (null == a || null == b) &#123; return null; &#125; return a + b;&#125;public Double evaluate(Double a, Double b) &#123; if (a == null || b == null) return null; return a + b; &#125;public Integer evaluate(Integer... a) &#123; int total = 0; for (int i = 0; i &lt; a.length; i++) if (a[i] != null) total += a[i]; return total; &#125;&#125; 2.GenericUDF相比较于UDF，具有更加丰富的功能，一个是可以接受和返回复杂数据类型了，例如Array什么的结构体类型，而不像UDF类那样只能是int、string之类的基本类型（当然真正代码中定义的是包装过的后缀为Writable的类型，但还是表示基本类型）；新的改进可以接受可变长度以及无限长度的参数了，因为可以用数组来表示输入参数了，而不需要像UDF类的实现类那样，要几种参数组合，就得重载几种方法；最重要的改进是可以通过DeferredObject类来实现所谓的”short-circuit“优化。数组的方式传递进参数，将会给用户开发自己的UDF提供极大的便捷性。不足的是GenericUDF的输出只能有一列，当然这个功能可以通过UDTF来弥补。下面给出一个例子讲解下GenericUDF的使用： 我将通过建立一个UDF函数：containsString，来加深对该API了解，该函数接收两个参数：一个String的列表（list）一个String根据该list中是否包含所提供的string来返回true或者false，如下：containsString(List(“a”, “b”, “c”), “b”); // true containsString(List(“a”, “b”, “c”), “d”); // false 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class ComplexUDFExample extends GenericUDF &#123; ListObjectInspector listOI; StringObjectInspector elementOI; @Override public String getDisplayString(String[] arg0) &#123; return "arrayContainsExample()"; // this should probably be better &#125; @Override public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException &#123; if (arguments.length != 2) &#123; throw new UDFArgumentLengthException("arrayContainsExample only takes 2 arguments: List&lt;T&gt;, T"); &#125; // 1. 检查是否接收到正确的参数类型 ObjectInspector a = arguments[0]; ObjectInspector b = arguments[1]; if (!(a instanceof ListObjectInspector) || !(b instanceof StringObjectInspector)) &#123; throw new UDFArgumentException("first argument must be a list / array, second argument must be a string"); &#125; this.listOI = (ListObjectInspector) a; this.elementOI = (StringObjectInspector) b; // 2. 检查list是否包含的元素都是string if(!(listOI.getListElementObjectInspector() instanceof StringObjectInspector)) &#123; throw new UDFArgumentException("first argument must be a list of strings"); &#125; // 返回类型是boolean，所以我们提供了正确的object inspector return PrimitiveObjectInspectorFactory.javaBooleanObjectInspector; &#125; @Override public Object evaluate(DeferredObject[] arguments) throws HiveException &#123; // 利用object inspectors从传递的对象中得到list与string List&lt;String&gt; list = (List&lt;String&gt;) this.listOI.getList(arguments[0].get()); String arg = elementOI.getPrimitiveJavaObject(arguments[1].get()); // 检查空值 if (list == null || arg == null) &#123; return null; &#125; // 判断是否list中包含目标值 for(String s: list) &#123; if (arg.equals(s)) return new Boolean(true); &#125; return new Boolean(false); &#125; &#125; 讲解： 1、该UDF用默认的构造器来初始化 2、udf.initialize() 被调用，传人udf参数的object instructors数组，（ListObjectInstructor, StringObjectInstructor）1) 检查传人的参数有两个与该参数的数据类型是正确的（见上面）2) 我们保存object instructors用以供evaluate()使用（listOI, elementOI）3) 返回 object inspector，让Hive能够读取该函数的返回结果（BooleanObjectInspector） 3、对于查询中的每一行，evaluate方法都会被调用，传人该行的指定的列（例如，evaluate(List(“a”, “b”, “c”), “c”) ）。1) 我们利用initialize方法中存储的object instructors来抽取出正确的值。2) 我们在这处理我们的逻辑然后用initialize返回的object inspector来序列化返回来的值(list.contains(elemement) ? true : false)。 UDAF1、Hive查询数据时，有些聚类函数在HQL没有自带，需要用户自定义实现。2、用户自定义聚合函数: Sum, Average…… n – 1UDAF（User- Defined Aggregation Funcation）一、用法1、两个包是必须import的 org.apache.hadoop.hive.ql.exec.UDAF和 org.apache.hadoop.hive.ql.exec.UDAFEvaluator。2、函数类需要继承UDAF类，内部类Evaluator实UDAFEvaluator接口。3、Evaluator需要实现 init、iterate、terminatePartial、merge、terminate这几个函数。a）init函数实现接口UDAFEvaluator的init函数。b）iterate接收传入的参数，并进行内部的轮转。其返回类型为boolean。c）terminatePartial无参数，其为iterate函数轮转结束后，返回轮转数据，terminatePartial类似于hadoop的Combiner。d）merge接收terminatePartial的返回结果，进行数据merge操作，其返回类型为boolean。e）terminate返回最终的聚集函数结果。使用方法如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445import org.apache.hadoop.hive.ql.exec.UDAF;import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;public class Avg extends UDAF &#123; public static class AvgState &#123; private long mCount; private double mSum; &#125; public static class AvgEvaluator implements UDAFEvaluator &#123; AvgState state; public AvgEvaluator() &#123; super(); state = new AvgState(); init(); &#125; /** * init函数类似于构造函数，用于UDAF的初始化 */ public void init() &#123; state.mSum = 0; state.mCount = 0; &#125; /** * iterate接收传入的参数，并进行内部的轮转。其返回类型为boolean * * @param o * @return */ public boolean iterate(Double o) &#123; if (o != null) &#123; state.mSum += o; state.mCount++; &#125; return true; &#125; /** * terminatePartial无参数，其为iterate函数轮转结束后，返回轮转数据， * terminatePartial类似于hadoop的Combiner * * @return */ public AvgState terminatePartial() &#123; // combiner return state.mCount == 0 ? null : state; &#125; /** * merge接收terminatePartial的返回结果，进行数据merge操作，其返回类型为boolean * * @param o * @return */ public boolean terminatePartial(Double o) &#123; if (o != null) &#123; state.mCount += o.mCount; state.mSum += o.mSum; &#125; return true; &#125; /** * terminate返回最终的聚集函数结果 * * @return */ public Double terminate() &#123; return state.mCount == 0 ? null : Double.valueOf(state.mSum / state.mCount); &#125; &#125;&#125; UDTFUDF与GenericUDF函数是操作单个数据域。它们必须要返回一个值。但是这并不适用于所用的数据处理任务。Hive可以存储许多类型的数据，而有时候我们并不想单数据域输入、单数据域输出。对于每一行的输入，可能我们想输出多行，又或是不输出，举个例子，想一下函数explode（一个hive内置函数）的作用。同样，可能我们也想输出多列，而不是输出单列。以上所有的要求我们可以用UDTF去完成。 示例：首先我们先假设我们想清洗people这张表中的人名，这个新的表有：1、姓和名 两个分开的列2、所有记录都包含姓名3、每条记录或有包含多个人名（eg Nick and Nicole Smith） 我门将使用继承org.apache.hadoop.hive.ql.udf.generic.GenericUDTF123456//该方法中，我们将指定输入输出参数：输入参数的ObjectInspector与输出参数的StructObjectInspector abstract StructObjectInspector initialize(ObjectInspector[] args) throws UDFArgumentException; //我们将处理一条输入记录，输出若干条结果记录 abstract void process(Object[] record) throws HiveException; //当没有记录处理的时候该方法会被调用，用来清理代码或者产生额外的输出 abstract void close() throws HiveException; 完整代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class NameParserGenericUDTF extends GenericUDTF &#123; private PrimitiveObjectInspector stringOI = null; @Override public StructObjectInspector initialize(ObjectInspector[] args) UDFArgumentException &#123; if (args.length != 1) &#123; throw new UDFArgumentException("NameParserGenericUDTF() takes exactly one argument"); &#125; if (args[0].getCategory() != ObjectInspector.Category.PRIMITIVE &amp;&amp; ((PrimitiveObjectInspector) args[0]).getPrimitiveCategory() != PrimitiveObjectInspector.PrimitiveCategory.STRING) &#123; throw new UDFArgumentException("NameParserGenericUDTF() takes a string as a parameter"); &#125; // 输入格式（inspectors） stringOI = (PrimitiveObjectInspector) args[0]; // 输出格式（inspectors） -- 有两个属性的对象 List&lt;String&gt; fieldNames = new ArrayList&lt;String&gt;(2); List&lt;ObjectInspector&gt; fieldOIs = new ArrayList&lt;ObjectInspector&gt;(2); fieldNames.add("name"); fieldNames.add("surname"); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs); &#125; public ArrayList&lt;Object[]&gt; processInputRecord(String name)&#123; ArrayList&lt;Object[]&gt; result = new ArrayList&lt;Object[]&gt;(); // 忽略null值与空值 if (name == null || name.isEmpty()) &#123; return result; &#125; String[] tokens = name.split("\\s+"); if (tokens.length == 2)&#123; result.add(new Object[] &#123; tokens[0], tokens[1] &#125;); &#125;else if (tokens.length == 4 &amp;&amp; tokens[1].equals("and"))&#123; result.add(new Object[] &#123; tokens[0], tokens[3] &#125;); result.add(new Object[] &#123; tokens[2], tokens[3] &#125;); &#125; return result; &#125; @Override public void process(Object[] record) throws HiveException &#123; final String name = stringOI.getPrimitiveJavaObject(record[0]).toString(); ArrayList&lt;Object[]&gt; results = processInputRecord(name); Iterator&lt;Object[]&gt; it = results.iterator(); while (it.hasNext())&#123; Object[] r = it.next(); forward(r); &#125; &#125; @Override public void close() throws HiveException &#123; // do nothing &#125; &#125; 讲解：该UDTF以string类型作为参数，返回一个拥有两个属性的对象，与GenericUDF比较相似，指定输入输出数据格式（objectinspector），以便hive能识别输入与输出。我们为输入的string参数定义了数据格式PrimitiveObjectInspector1stringOI = (PrimitiveObjectInspector) args[0] 定义输出数据格式（objectinspectors）需要我们先定义两个属性名称，因为（objectinspectors）需要读取每一个属性（在这个实例中，两个属性都是string类型）。123456789List&lt;String&gt; fieldNames = new ArrayList&lt;String&gt;(2); fieldNames.add("name"); fieldNames.add("surname"); List&lt;ObjectInspector&gt; fieldOIs = new ArrayList&lt;ObjectInspector&gt;(2); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs); 我们主要的处理逻辑放在这个比较直观的processInputRecord方法当中。分开逻辑处理有利我们进行更简单的单元测试，而不用涉及到繁琐的objectinspector。最后，一旦得到结果就可以对其进行forward，将其注册为hive处理后的输出记录对象。 12345while (it.hasNext())&#123; Object[] r = it.next(); forward(r); &#125; &#125; 三、如何在Hive中使用UDF无论是UDF、UDTF、还是UDAF其使用方法都是一样的。 在hive中添加jar包： 12hive&gt;add jar &lt;your jar filePath&gt;如：add jar /home/hdfs/myUdf.jar 创建临时函数 12hive&gt;create temporary function avg_test as '&lt;类的全路径&gt;'如： create temporary function avg_test as 'hive.udaf.Avg' 查询临时函数的注释信息如果在编写UDF时，你写了相关的注释信息，只需要使用如下命令就可以查看到： 12345查询可以使用的funtion列表hive&gt;SHOW FUNCTIONS; 查询具体的function信息hive&gt;desc function &lt;your function name&gt;如： desc function avg_test 使用临时函数创建完成的temporary function使用时和hive内置的function没有区别，如 1hive&gt;select avg_test(scores.math) from scores; 销毁临时函数 1hive&gt;drop temporary function avg_test; 最后 Hive2.1中使用GenericUDF时，使用了一些技巧，在其基础上封装了BaseMaskUDF，虽然不能被直接使用，但是可以将其复制出来为己所用，下章将会讲解下BaseMaskUDF，完成后将会把地址放在文下。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[关联规则-FP-Growth]]></title>
      <url>%2F2017%2F05%2F14%2F%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99-FP-Growth%2F</url>
      <content type="text"><![CDATA[发现有篇博客对Fp-Growth讲解的很清楚，很赞，原文链接为：http://www.cnblogs.com/datahunter/p/3903413.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[关联规则-Apriori算法]]></title>
      <url>%2F2017%2F05%2F14%2F%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99-Apriori%E7%AE%97%E6%B3%95%2F</url>
      <content type="text"><![CDATA[关联规则的挖掘分为两步：1，找出所有频繁项集；2，由频繁项集产生强关联规则。总体的性能由第一步决定。Apriori核心算法思想简要描述如下：该算法中有两个关键步骤为连接步和剪枝步。1）连接步：为找出Lk(频繁k项集)，通过Lk-1与自身连接，产生候选k项集，该候选k项集，该候选集记作Ck；其中Lk-1的元素是可连接的。2）剪枝步：Ck是Lk的超集，即它的成员可以是也可以不是频繁的，但所有的频繁项集都包含在Ck中。扫描数据库，确定Ck中每一个候选的计数，从而确定Lk（计数值步小于最小支持度计数的所有候选是频繁的，从而属于Lk）。然而，Ck可能很大，这样涉及的计算量就很大。为了压缩Ck，使用Apriori性质：任何非频繁的（k-1）项集都不可能是频繁k项集的子集（推论出：频繁k项集的任何非空真子集都必需是频繁的）。因此，如果一个候选K项集（k-1）项集不在Lk中，则该候选项不可能是频繁的，从而可以由Ck中删除（可利用该定论来剪枝：只需检验（k-1）项集是不是频繁的就可以了，因为小于k-1项集已经在得到（k-1）项集时得到了验证）。 其中比较容易产生疑问的是自连接这里，过程可以用下图表示： 算法步骤： 扫描全部数据，产生候选1-项集的集合C1; 根据最小支持度，由候选1-项集的集合C1产生频繁1-项集的集合L1； 对k&gt;1，重复执行步骤4、5、6 由Lk执行连接（自连接）和剪枝操作，产生候选（k+1）-项集的集合Ck+1； 根据最小支持度，由候选（k+1）-项集的集合Ck+1，产生频繁（k+1）-项集的集合Lk+1； 若L!=空集，则k=k+1，跳往步骤4；否则跳往步骤7； 根据最小置信度，由频繁项集产生强关联规则，结束。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习初涉-高斯混合模型聚类]]></title>
      <url>%2F2017%2F05%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%B6%89-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E8%81%9A%E7%B1%BB%2F</url>
      <content type="text"><![CDATA[转载自： http://blog.pluskid.org/?p=39 漫谈 Clustering (3): Gaussian Mixture Model 实上，GMM 和 k-means 很像，不过 GMM 是学习出一些概率密度函数来（所以 GMM 除了用在 clustering 上之外，还经常被用于 density estimation ），简单地说，k-means 的结果是每个数据点被 assign 到其中某一个 cluster 了，而 GMM 则给出这些数据点被 assign 到每个 cluster 的概率，又称作 soft assignment 。得出一个概率有很多好处，因为它的信息量比简单的一个结果要多，比如，我可以把这个概率转换为一个 score ，表示算法对自己得出的这个结果的把握。也许我可以对同一个任务，用多个方法得到结果，最后选取“把握”最大的那个结果；另一个很常见的方法是在诸如疾病诊断之类的场所，机器对于那些很容易分辨的情况（患病或者不患病的概率很高）可以自动区分，而对于那种很难分辨的情况，比如，49% 的概率患病，51% 的概率正常，如果仅仅简单地使用 50% 的阈值将患者诊断为“正常”的话，风险是非常大的，因此，在机器对自己的结果把握很小的情况下，会“拒绝发表评论”，而把这个任务留给有经验的医生去解决。 我们已经使用过k-means算法解决聚类问题。这个算法的突出优点是简单易用，计算量也不多。然而，往往过于简单也是一个缺点。假设聚类可以表示为单个点往往会过于粗糙。举一个例子，如下图所示： 这个例子中数据位于同心圆。在这种情况下，标准的K均值由于两个圆的均值位置相同，无法把数据划分成簇（所以上面有一个绿点不知道该往哪跑，因为它没有簇）。因此，以距离模型为聚类标准的方法不一定都能成功适用。为了解决这些缺点，我们介绍一种用统计混合模型进行聚类的方法——高斯混合模型（Gaussian Mixture Model, GMM）。这种聚类方法得到的是每个样本点属于各个类的概率，而不是判定它完全属于一个类，所以有时也会被称为软聚类。 废话说了一堆，不过，在回到 GMM 之前，我们再稍微扯几句。我们知道，不管是机器还是人，学习的过程都可以看作是一种“归纳”的过程，在归纳的时候你需要有一些假设的前提条件，例如，当你被告知水里游的那个家伙是鱼之后，你使用“在同样的地方生活的是同一种东西”这类似的假设，归纳出“在水里游的都是鱼”这样一个结论。当然这个过程是完全“本能”的，如果不仔细去想，你也不会了解自己是怎样“认识鱼”的。另一个值得注意的地方是这样的假设并不总是完全正确的，甚至可以说总是会有这样那样的缺陷的，因此你有可能会把虾、龟、甚至是潜水员当做鱼。也许你觉得可以通过修改前提假设来解决这个问题，例如，基于“生活在同样的地方并且穿着同样衣服的是同一种东西”这个假设，你得出结论：在水里有并且身上长有鳞片的是鱼。可是这样还是有问题，因为有些没有长鳞片的鱼现在又被你排除在外了。 在这个问题上，机器学习面临着和人一样的问题，在机器学习中，一个学习算法也会有一个前提假设，这里被称作“归纳偏执 (bias)”（bias 这个英文词在机器学习和统计里还有其他许多的意思）。例如线性回归，目的是要找一个函数尽可能好地拟合给定的数据点，它的归纳偏执就是“满足要求的函数必须是线性函数”。一个没有归纳偏执的学习算法从某种意义上来说毫无用处，就像一个完全没有归纳能力的人一样，在第一次看到鱼的时候有人告诉他那是鱼，下次看到另一条鱼了，他并不知道那也是鱼，因为两条鱼总有一些地方不一样的，或者就算是同一条鱼，在河里不同的地方看到，或者只是看到的时间不一样，也会被他认为是不同的，因为他无法归纳，无法提取主要矛盾、忽略次要因素，只好要求所有的条件都完全一样──然而哲学家已经告诉过我们了：世界上不会有任何样东西是完全一样的，所以这个人即使是有无比强悍的记忆力，也绝学不到任何一点知识。 这个问题在机器学习中称作“过拟合 (Overfitting)”，例如前面的回归的问题，如果去掉“线性函数”这个归纳偏执，因为对于 N 个点，我们总是可以构造一个 N-1 次多项式函数，让它完美地穿过所有的这 N 个点，或者如果我用任何大于 N-1 次的多项式函数的话，我甚至可以构造出无穷多个满足条件的函数出来。如果假定特定领域里的问题所给定的数据个数总是有个上限的话，我可以取一个足够大的 N ，从而得到一个（或者无穷多个）“超级函数”，能够 fit 这个领域内所有的问题。然而这个（或者这无穷多个）“超级函数”有用吗？只要我们注意到学习的目的（通常）不是解释现有的事物，而是从中归纳出知识，并能应用到新的事物上，结果就显而易见了。 没有归纳偏执或者归纳偏执太宽泛会导致 Overfitting ，然而另一个极端──限制过大的归纳偏执也是有问题的：如果数据本身并不是线性的，强行用线性函数去做回归通常并不能得到好结果。难点正在于在这之间寻找一个平衡点。不过人在这里相对于（现在的）机器来说有一个很大的优势：人通常不会孤立地用某一个独立的系统和模型去处理问题，一个人每天都会从各个来源获取大量的信息，并且通过各种手段进行整合处理，归纳所得的所有知识最终得以统一地存储起来，并能有机地组合起来去解决特定的问题。这里的“有机”这个词很有意思，搞理论的人总能提出各种各样的模型，并且这些模型都有严格的理论基础保证能达到期望的目的，然而绝大多数模型都会有那么一些“参数”（例如 K-means 中的 k ），通常没有理论来说明参数取哪个值更好，而模型实际的效果却通常和参数是否取到最优值有很大的关系，我觉得，在这里“有机”不妨看作是所有模型的参数已经自动地取到了最优值。另外，虽然进展不大，但是人们也一直都期望在计算机领域也建立起一个统一的知识系统（例如语意网就是这样一个尝试）。 废话终于说完了，回到 GMM 。按照我们前面的讨论，作为一个流行的算法，GMM 肯定有它自己的一个相当体面的归纳偏执了。其实它的假设非常简单，顾名思义，Gaussian Mixture Model ，就是假设数据服从 Mixture Gaussian Distribution ，换句话说，数据可以看作是从数个 Gaussian Distribution 中生成出来的。实际上，我们在 K-means 和 K-medoids 两篇文章中用到的那个例子就是由三个 Gaussian 分布从随机选取出来的。实际上，从中心极限定理可以看出，Gaussian 分布（也叫做正态 (Normal) 分布）这个假设其实是比较合理的，除此之外，Gaussian 分布在计算上也有一些很好的性质，所以，虽然我们可以用不同的分布来随意地构造 XX Mixture Model ，但是还是 GMM 最为流行。另外，Mixture Model 本身其实也是可以变得任意复杂的，通过增加 Model 的个数，我们可以任意地逼近任何连续的概率密分布。 每个 GMM 由 K 个 Gaussian 分布组成，每个 Gaussian 称为一个“Component”，这些 Component 线性加成在一起就组成了 GMM 的概率密度函数：根据上面的式子，如果我们要从 GMM 的分布中随机地取一个点的话，实际上可以分为两步：首先随机地在这 K 个 Component 之中选一个，每个 Component 被选中的概率实际上就是它的系数 πk ,选中了 Component 之后，再单独地考虑从这个 Component 的分布中选取一个点就可以了──这里已经回到了普通的 Gaussian 分布，转化为了已知的问题。 那么如何用 GMM 来做 clustering 呢？其实很简单，现在我们有了数据，假定它们是由 GMM 生成出来的，那么我们只要根据数据推出 GMM 的概率分布来就可以了，然后 GMM 的 K 个 Component 实际上就对应了 K 个 cluster 了。根据数据来推算概率密度通常被称作 density estimation ，特别地，当我们在已知（或假定）了概率密度函数的形式，而要估计其中的参数的过程被称作“参数估计”。 现在假设我们有 N 个数据点，并假设它们服从某个分布（记作 p(x) ），现在要确定里面的一些参数的值，例如，在 GMM 中，我们就需要确定 \pi_k、\mu_k 和 \Sigma_k 这些参数。 我们的想法是，找到这样一组参数，它所确定的概率分布生成这些给定的数据点的概率最大，而这个概率实际上就等于我们把这个乘积称作似然函数 (Likelihood Function)。通常单个点的概率都很小，许多很小的数字相乘起来在计算机里很容易造成浮点数下溢，因此我们通常会对其取对数，把乘积变成加和得到 log-likelihood function 。接下来我们只要将这个函数最大化（通常的做法是求导并令导数等于零，然后解方程），亦即找到这样一组参数值，它让似然函数取得最大值，我们就认为这是最合适的参数，这样就完成了参数估计的过程。 下面让我们来看一看 GMM 的 log-likelihood function： 由于在对数函数里面又有加和，我们没法直接用求导解方程的办法直接求得最大值。为了解决这个问题，我们采取之前从 GMM 中随机选点的办法：分成两步，实际上也就类似于 K-means 的两步。 估计数据由每个 Component 生成的概率（并不是每个 Component 被选中的概率）：对于每个数据 Xi 来说，它由第 k 个 Component 生成的概率为由于式子里的 μk 和 Σk 也是需要我们估计的值，我们采用迭代法，在计算 γ(i, k) 的时候我们假定 μk 和 Σk 均已知，我们将取上一次迭代所得的值（或者初始值）。 估计每个 Component 的参数：现在我们假设上一步中得到的 γ(i, k) 就是正确的“数据 Xi 由 Component k 生成的概率”，亦可以当做该 Component 在生成这个数据上所做的贡献，或者说，我们可以看作Xi 这个值其中有 γ(i, k)Xi 这部分是由 Component k 所生成的。集中考虑所有的数据点，现在实际上可以看作 Component 生成了 γ(1, k)X1, … , γ(n, k)Xn 这些点。由于每个 Component 都是一个标准的 Gaussian 分布，可以很容易分布求出最大似然所对应的参数值：其中 ，并且 πk 也顺理成章地可以估计为 Nk/N 。 重复迭代前面两步，直到似然函数的值收敛为止。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习初涉--层次聚类]]></title>
      <url>%2F2017%2F05%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%B6%89-%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%2F</url>
      <content type="text"><![CDATA[&emsp;&emsp;层次聚类算法，是通过将数据组织为若干组并形成一个相应的树来进行聚类的。 层次聚类可以自底向上也可以自顶向下，自顶向下的叫分裂的聚类算法，自底向上的叫凝聚的聚类算法。&emsp;&emsp;一个简单的凝聚的层次聚类过程如图所示，一个完全层次聚类的质量由于无法对已经做的合并或分解进行调整而受到影响（简单的说就是一个点被划分到一个簇，或者两个簇合并后，后续无法进行改变，将会一直影响到聚类结束）。但是层次聚类算法没有使用准则函数，它所含的对数据结构的假设更少，所以它的通用性更强。&emsp;&emsp;凝聚的层次聚类：自底向上的策略，首先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇，直到所有的对象都在一个簇中或者达到某个终结条件。&emsp;&emsp;分裂的层次聚类：顶向下的策略，首先将所有的簇都放在一个簇中，然后慢慢地细分为越来越小的簇，直到每个对象自成一簇或者达到其他的一个终结条件，例如满足了某个期望的簇数目，又或者两个最近的簇之间的距离达到了某一个阈值。说明：在凝聚的层次聚类中，两个簇之间的相似度是由这里的两个不同簇中的距离最近的数据点的数据点对的相似度来定义的。在分裂的层次聚类中，簇根据某个准则进行分裂，例如这个簇中的A和B之间的距离是这个簇中所有样本间距离最远的一对，分类时以样本A和B作为新的簇，其他的样本根据距离A和B的远近划分到对应的簇中。 特点 用户需要提供所希望得到的聚类的单个数量和阈值作为结束条件，而这个条件对于复杂数据来说是难以判定的。 实现原理简单，但是偶尔会遇见合并或分裂点的抉择的困难。 已形成的处理就不能撤销，两个聚类之间也不能交换对象。 不具有很好的伸缩性，因为分类或合并时的决策需要经过检测和估算大量的对象和簇。 层次聚类算法由于使用距离矩阵，时间和空间复杂度都很高O(N^2),几乎不能在大数据集上使用。 层次聚类算法只处理符合某静态模型的簇忽略了不同簇间的信息而且忽略了粗剪的互联性（粗剪距离较近的数据对的多少）和近似度（簇间对数据对的相似度）。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习初涉--k近邻算法]]></title>
      <url>%2F2017%2F05%2F10%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%87%BA%E6%B6%89-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%2F</url>
      <content type="text"><![CDATA[在此对k近邻算法做一个简单的总结： 原理K-近邻方法通过计算每个训练样例到待分类样品的距离，取和待分类样品距离最近的k个训练样例，k个样品中哪个类别的训练样例占多数，则待分类原组就属于哪个类别。在实践中往往通过若干次实验来确定K值，取分类误差率最小的K值。 特点KNN方法主要依靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别，因此对于类域的交叉或者重叠较多的待分类样本集来说，KNN方法更为合适。不足之处是计算量较大，因为对每一个待分类的样本都要计算它到全体已知样本的距离，才能求得它的K个最邻近点。 改进:对于计算量大的问题目前常用的解决办法是事先对已知样本点进行剪辑，实现去除对分类作用不大的样本。对样本进行组织与整理，分群分层，尽可能地将计算压缩在接近测试样本领域的小范围内。 总的来说，算法的适应性较强，尤其适用于样本容量较大的自动分类问题，而那些样本容量较小的分类问题采用这种算法比较容易产生误分。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习初涉—-决策树分类]]></title>
      <url>%2F2017%2F05%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%B6%89%E2%80%94-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%2F</url>
      <content type="text"><![CDATA[决策树分类我在csdn上进行过介绍，传送门：决策树分类算法http://blog.csdn.net/u013668852/article/details/52935169]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[动态规划-DAG-硬币问题]]></title>
      <url>%2F2017%2F04%2F17%2F%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-DAG-%E7%A1%AC%E5%B8%81%E9%97%AE%E9%A2%98%2F</url>
      <content type="text"><![CDATA[题目：有n种硬币，面值分别为V1,V2,…Vn,每种都有无限多。给定非负整数S，可以选用多少个硬币，使得面值之和恰好为S？输出硬币数目的最小值和最大值！ &emsp;&emsp;如果我们有面值为1元、3元和5元的硬币若干枚，如何用最少的硬币凑够11元？ (表面上这道题可以用贪心算法，但贪心算法无法保证可以求出解，比如1元换成2元的时候) &emsp;&emsp;首先我们思考一个问题，如何用最少的硬币凑够i元(i&lt;11)？为什么要这么问呢？两个原因：1.当我们遇到一个大问题时，总是习惯把问题的规模变小，这样便于分析讨论。2.这个规模变小后的问题和原来的问题是同质的，除了规模变小，其它的都是一样的，本质上它还是同一个问题(规模变小后的问题其实是原问题的子问题)。 &emsp;&emsp;好了，让我们从最小的i开始吧。当i=0，即我们需要多少个硬币来凑够0元。由于1，3，5都大于0，即没有比0小的币值，因此凑够0元我们最少需要0个硬币。这时候我们发现用一个标记来表示这句“凑够0元我们最少需要0个硬币。 &emsp;&emsp;那么， 我们用d(i)=j来表示凑够i元最少需要j个硬币。于是我们已经得到了d(0)=0，表示凑够0元最小需要0个硬币。当i=1时，只有面值为1元的硬币可用，因此我们拿起一个面值为1的硬币，接下来只需要凑够0元即可，而这个是已经知道答案的，即d(0)=0。所以，d(1)=d(1-1)+1=d(0)+1=0+1=1。 当i=2时， 仍然只有面值为1的硬币可用，于是我拿起一个面值为1的硬币，接下来我只需要再凑够2-1=1元即可(记得要用最小的硬币数量)，而这个答案也已经知道了。所以d(2)=d(2-1)+1=d(1)+1=1+1=2。 &emsp;&emsp;一直到这里，你都可能会觉得，好无聊，感觉像做小学生的题目似的。因为我们一直都只能操作面值为1的硬币！耐心点，让我们看看i=3时的情况。当i=3时，我们能用的硬币就有两种了：1元的和3元的(5元的仍然没用，因为你需要凑的数目是3元！5元太多了亲)。既然能用的硬币有两种，我就有两种方案。如果我拿了一个1元的硬币，我的目标就变为了:凑够3-1=2元需要的最少硬币数量。即d(3)=d(3-1)+1=d(2)+1=2+1=3。这个方案说的是，我拿3个1元的硬币；第二种方案是我拿起一个3元的硬币，我的目标就变成：凑够3-3=0元需要的最少硬币数量。即d(3)=d(3-3)+1=d(0)+1=0+1=1. &emsp;&emsp;这个方案说的是，我拿1个3元的硬币。好了，这两种方案哪种更优呢？ 记得我们可是要用最少的硬币数量来凑够3元的。所以， 选择d(3)=1，怎么来的呢？ &emsp;&emsp;具体是这样得到的：d(3)=min{d(3-1)+1, d(3-3)+1}。 &emsp;&emsp;上文中d(i)表示凑够i元需要的最少硬币数量，我们将它定义为该问题的”状态”，这个状态是怎么找出来的呢？我在另一篇文章中写过：根据子问题定义状态。你找到子问题，状态也就浮出水面了。 &emsp;&emsp;最终我们要求解的问题，可以用这个状态来表示：d(11)，即凑够11元最少需要多少个硬币。 那状态转移方程是什么呢？既然我们用d(i)表示状态，那么状态转移方程自然包含d(i)， 上文中包含状态d(i)的方程是：d(3)=min{d(3-1)+1,d(3-3)+1}。没错，它就是状态转移方程，描述状态之间是如何转移的。当然，我们要对它抽象一下，d(i)=min{ d(i-vj)+1 }，其中i-vj &gt;=0，vj表示第j个硬币的面值; &emsp;&emsp;有了状态和状态转移方程，这个问题基本上也就解决了。 123456789101112131415161718192021222324252627282930313233#include&lt;cstdio&gt; #include&lt;cstring&gt; int d[1000],v[10]; int max(int a, int b) &#123; return a &gt; b ? a : b; &#125; int dpmax(int s,int n) &#123; if(s&lt;0) return -2&lt;&lt;30; if (d[s] != -1) return d[s]; d[s] = -2&lt;&lt;30;//用于设定不能走到终点的路肯定小于能走到终点的路 for (int i = 0;i &lt; n;i++) if (s &gt;= v[i])//等号很关键 //根据公式：max&#123;dpmax(s-v[i])+1&#125;,代码max(d[s], dpmax(s - v[i], n)+1)中的d[s]记录的是上一个值 d[s] = max(d[s], dpmax(s - v[i], n)+1); return d[s]; &#125; int main() &#123; int n, s, i; while (scanf("%d%d", &amp;n, &amp;s) != EOF) &#123; for (i = 0;i &lt; n;i++) scanf("%d", &amp;v[i]); memset(d, -1, sizeof d); d[0] = 0;//用于辨别该路能否走到终点 printf("%d\n", dpmax(s, n)); &#125; return 0; &#125; 解答转载自http://www.cnblogs.com/sunTin/p/6674945.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[maven打造可执行war包]]></title>
      <url>%2F2017%2F04%2F07%2Fmaven%E6%89%93%E9%80%A0%E5%8F%AF%E6%89%A7%E8%A1%8Cjar%E5%8C%85%2F</url>
      <content type="text"><![CDATA[在开发java Web时，有时我们会使用嵌入式jetty来运行，项目完成后，如果能够直接运行war包从而启动jetty来运行war包那就非常完美了，本文将讲解如何在项目中整合jetty 9，并构造可执行的war包（打包前和打包后都能随时启动）。1.首先添加jetty 9的依赖（本文暂时只用到了jetty的以下依赖，读者根据自己的项目需要增加）12345678910&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-server&lt;/artifactId&gt; &lt;version&gt;9.2.7.v20150116&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-webapp&lt;/artifactId&gt; &lt;version&gt;9.2.7.v20150116&lt;/version&gt;&lt;/dependency&gt; 2.项目中使用jetty 9。 首先我封装了自己的JettyServer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public class EmbeddedServer &#123; //public static final Logger logger = LoggerFactory.getLogger(EmbeddedServer.class); // private static final int DEFAULT_BUFFER_SIZE = 16192; protected final Server server = new Server(); public EmbeddedServer(int port,String path) throws IOException&#123; this(port,path,false,null); &#125; /** * use war to start * @param port * @param isWar * @param warPath * @throws IOException */ public EmbeddedServer(int port,boolean isWar,String warPath) throws IOException&#123; this(port,null,isWar,warPath); &#125; private EmbeddedServer(int port, String path,boolean isWar,String warPath) throws IOException &#123; Connector connector = getConnector(port); server.addConnector(connector); WebAppContext application = getWebAppContext(path,isWar,warPath); server.setHandler(application); server.setStopAtShutdown(true); &#125; protected WebAppContext getWebAppContext(String path,boolean isWar,String warPath) &#123; WebAppContext application; if(isWar)&#123; application=new WebAppContext(); application.setWar(warPath); return application; &#125;else&#123; application = new WebAppContext(path, "/"); application.setConfigurationDiscovered(true); application.setParentLoaderPriority(true); application.setClassLoader(Thread.currentThread().getContextClassLoader()); return application; &#125; &#125; protected Connector getConnector(int port) throws IOException &#123; HttpConfiguration http_config = new HttpConfiguration(); // this is to enable large header sizes when Kerberos is enabled with AD //final int bufferSize = getBufferSize(); //http_config.setResponseHeaderSize(bufferSize); //http_config.setRequestHeaderSize(bufferSize); ServerConnector connector = new ServerConnector(server, new HttpConnectionFactory(http_config)); connector.setPort(port); connector.setHost("0.0.0.0"); server.addConnector(connector); return connector; &#125; /*protected Integer getBufferSize() &#123; try &#123; Configuration configuration = ApplicationProperties.get(); return configuration.getInt("sysimple.jetty.request.buffer.size", DEFAULT_BUFFER_SIZE); &#125; catch (Exception e) &#123; // do nothing &#125; return DEFAULT_BUFFER_SIZE; &#125;*/ public void start() throws Exception &#123; server.start(); //logger.info("********************************************************"); //logger.info("The SySimple Has Started !!!"); server.join(); &#125; public void stop() &#123; try &#123; server.stop(); &#125; catch (Exception e) &#123; //logger.warn("Error during shutdown", e); &#125; &#125;&#125; 接着可以使用封装好的EmbeddedServer来启动war 12345678910111213141516171819public class StartWeb&#123; private static EmbeddedServer embeddedServer; public static void main(String[] args)&#123; //Start web server int port=3000； try&#123; if(args.length==0)&#123; //该方式能够在开发时快速启动 embeddedServer=new EmbeddedServer(port, "src/main/webapp"); &#125;else&#123; //传入war包的路径，该方法能够在打包完成后启动该war包 embeddedServer=new EmbeddedServer(port, true, args[0]); &#125; embeddedServer.start(); &#125;catch(Exception e)&#123; System.exit(0); &#125; &#125;&#125; 注意：打包后如果需要启动war包，需要使用如下的这种批处理命令来启动： 以批处理命令（start.bat）和server.war在同级目录下为例：（以下是start.bat的内容） @echo off set bat_dir=%~dp0 java -jar %bat_dir%/web.war %bat_dir%/web.war 读者可以考虑在代码中得到war包的路径，这样可以在启动时省去传参。 下面是最重要的：使用Maven构建可执行war包 总的来说可执行war包是将war包的结构仿照jar包的结构进行改变，第一个是需要在manifest中标记出主方法，第二个是编译后的代码（包，而非.class）必须放在war包的最外层，最后要能够找到项目的依赖。 ①标记主方法 通过maven-war-plugin在manifest中标记主方法入口 1234567891011&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;org.bit.linc.web.commons.StartWeb&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/plugin&gt; ②拷贝（也可以移动）web的所有的代码到war包最外层（使用maven-antrun-plugin） 12345678910111213141516171819202122&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;main-class-placement&lt;/id&gt; &lt;phase&gt;prepare-package&lt;/phase&gt; &lt;configuration&gt; &lt;target&gt; &lt;copy todir="$&#123;project.build.directory&#125;/$&#123;project.artifactId&#125;/"&gt; &lt;fileset dir="$&#123;project.build.directory&#125;/classes/"&gt; &lt;include name="**/*.*" /&gt; &lt;/fileset&gt; &lt;/copy&gt; &lt;/target&gt; &lt;/configuration&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; ③ 标记所有依赖的位置（将代码拷贝到war最外层后，会出现依赖的类都找不到的情况，因此需要让war包能够查找到这些依赖） 将maven-war-plugin更改为如下内容： 12345678910111213&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;org.bit.linc.web.commons.StartWeb&lt;/mainClass&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;classpathPrefix&gt;WEB-INF/lib&lt;/classpathPrefix&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/plugin&gt; 现在可以构建可执行war包了。 以笔者的项目为例：构建的war包中META-INF/MANIFEST.MF会变成如下内容： Manifest-Version: 1.0 Built-By: wubo Build-Jdk: 1.7.0_17 Class-Path: WEB-INF/lib/commons-0.0.2.jar WEB-INF/lib/commons-configur ation-1.8.jar WEB-INF/lib/commons-lang-2.6.jar WEB-INF/lib/commons-lo gging-1.1.1.jar WEB-INF/lib/slf4j-api-1.7.7.jar WEB-INF/lib/slf4j-log 4j12-1.7.7.jar WEB-INF/lib/log4j-1.2.17.jar WEB-INF/lib/plugins-0.0.2 .jar WEB-INF/lib/clusters-0.0.2.jar WEB-INF/lib/monitors-0.0.2.jar WE B-INF/lib/jetty-server-9.2.7.v20150116.jar WEB-INF/lib/javax.servlet- api-3.1.0.jar WEB-INF/lib/jetty-http-9.2.7.v20150116.jar WEB-INF/lib/ jetty-util-9.2.7.v20150116.jar WEB-INF/lib/jetty-io-9.2.7.v20150116.j ar WEB-INF/lib/jetty-webapp-9.2.7.v20150116.jar WEB-INF/lib/jetty-xml -9.2.7.v20150116.jar WEB-INF/lib/jetty-servlet-9.2.7.v20150116.jar WE B-INF/lib/jetty-security-9.2.7.v20150116.jar WEB-INF/lib/gson-2.3.1.j ar Created-By: Apache Maven 3.3.9 Main-Class: org.bit.linc.web.commons.StartWeb Archiver-Version: Plexus Archiver 其中的Class-Path和Main-Class均已经改变。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习初涉--贝叶斯分类]]></title>
      <url>%2F2017%2F04%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%B6%89-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%2F</url>
      <content type="text"><![CDATA[贝叶斯定理 &emsp;&emsp;这个定理解决了现实生活里经常遇到的问题：已知某条件概率，如何得到两个事件交换后的概率，也就是在已知P(A|B)的情况下如何求得P(B|A)。这里先解释什么是条件概率： &emsp;&emsp;P(A|B) 表示事件B已经发生的前提下，事件A发生的概率，叫做事件B发生下事件A的条件概率。其基本求解公式为：。 &emsp;&emsp;贝叶斯定理之所以有用，是因为我们在生活中经常遇到这种情况：我们可以很容易直接得出P(A|B)，P(B|A)则很难直接得出，但我们更关心P(B|A)，贝叶斯定理就为我们打通从P(A|B)获得P(B|A)的道路。 &emsp;&emsp;下面不加证明地直接给出贝叶斯定理： &emsp;&emsp; 朴素贝叶斯分类朴素贝叶斯分类的原理与流程 &emsp;&emsp;朴素贝叶斯分类是一种十分简单的分类算法，叫它朴素贝叶斯分类是因为这种方法的思想真的很朴素，朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。通俗来说，就好比这么个道理，你在街上看到一个黑人，我问你你猜这哥们哪里来的，你十有八九猜非洲。为什么呢？因为黑人中非洲人的比率最高，当然人家也可能是美洲人或亚洲人，但在没有其它可用信息下，我们会选择条件概率最大的类别，这就是朴素贝叶斯的思想基础。 &emsp;&emsp;朴素贝叶斯分类的正式定义如下：1、设 为一个待分类项，而每个a为x的一个特征属性。2、有类别集合3、计算4、如果，则。 &emsp;&emsp;那么现在的关键就是如何计算第3步中的各个条件概率。我们可以这么做：1、找到一个已知分类的待分类项集合，这个集合叫做训练样本集。2、统计得到在各类别下各个特征属性的条件概率估计。即3、如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有：根据上述分析，朴素贝叶斯分类的流程可以由下图表示（暂时不考虑验证）： 可以看到，整个朴素贝叶斯分类分为三个阶段： &emsp;&emsp;第一阶段——准备工作阶段，这个阶段的任务是为朴素贝叶斯分类做必要的准备，主要工作是根据具体情况确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本集合。这一阶段的输入是所有待分类数据，输出是特征属性和训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。 &emsp;&emsp;第二阶段——分类器训练阶段，这个阶段的任务就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计，并将结果记录。其输入是特征属性和训练样本，输出是分类器。这一阶段是机械性阶段，根据前面讨论的公式可以由程序自动计算完成。 &emsp;&emsp;第三阶段——应用阶段。这个阶段的任务是使用分类器对待分类项进行分类，其输入是分类器和待分类项，输出是待分类项与类别的映射关系。这一阶段也是机械性阶段，由程序完成。 估计类别下特征属性划分的条件概率及Laplace校准 &emsp;&emsp;这一节讨论P(a|y)的估计。 &emsp;&emsp;由上文看出，计算各个划分的条件概率P(a|y)是朴素贝叶斯分类的关键性步骤，当特征属性为离散值时，只要很方便的统计训练样本中各个划分在每个类别中出现的频率即可用来估计P(a|y)，下面重点讨论特征属性是连续值的情况。 &emsp;&emsp;当特征属性为连续值时，通常假定其值服从高斯分布（也称正态分布）。即： &emsp;&emsp;而 &emsp;&emsp;因此只要计算出训练样本中各个类别中此特征项划分的各均值和标准差，代入上述公式即可得到需要的估计值。均值与标准差的计算在此不再赘述。 &emsp;&emsp;另一个需要讨论的问题就是当P(a|y)=0怎么办，当某个类别下某个特征项划分没有出现时，就是产生这种现象，这会令分类器质量大大降低。为了解决这个问题，我们引入Laplace校准，它的思想非常简单，就是对没类别下所有划分的计数加1，这样如果训练样本集数量充分大时，并不会对结果产生影响，并且解决了上述频率为0的尴尬局面。 朴素贝叶斯分类器的应用一、病人分类的例子 &emsp;&emsp;某个医院早上收了六个门诊病人，如下表。 1234567症状 职业 疾病打喷嚏 护士 感冒 打喷嚏 农夫 过敏 头痛 建筑工人 脑震荡 头痛 建筑工人 感冒 打喷嚏 教师 感冒 头痛 教师 脑震荡 &emsp;&emsp;现在又来了第七个病人，是一个打喷嚏的建筑工人。请问他患上感冒的概率有多大？根据贝叶斯定理： 1P(A|B) = P(B|A) P(A) / P(B) &emsp;&emsp;可得 123P(感冒|打喷嚏x建筑工人) = P(打喷嚏x建筑工人|感冒) x P(感冒) / P(打喷嚏x建筑工人) &emsp;&emsp;假定”打喷嚏”和”建筑工人”这两个特征是独立的，因此，上面的等式就变成了 123P(感冒|打喷嚏x建筑工人) = P(打喷嚏|感冒) x P(建筑工人|感冒) x P(感冒) / P(打喷嚏) x P(建筑工人) &emsp;&emsp;这是可以计算的。 123P(感冒|打喷嚏x建筑工人) = 0.66 x 0.33 x 0.5 / 0.5 x 0.33 = 0.66 &emsp;&emsp;因此，这个打喷嚏的建筑工人，有66%的概率是得了感冒。同理，可以计算这个病人患上过敏或脑震荡的概率。比较这几个概率，就可以知道他最可能得什么病。这就是贝叶斯分类器的基本方法：在统计资料的基础上，依据某些特征，计算各个类别的概率，从而实现分类。 二、账号分类的例子&emsp;&emsp;根据某社区网站的抽样统计，该站10000个账号中有89%为真实账号（设为C0），11%为虚假账号（设为C1）。 12C0 = 0.89 C1 = 0.11 &emsp;&emsp;接下来，就要用统计资料判断一个账号的真实性。假定某一个账号有以下三个特征： 123456F1: 日志数量/注册天数 F2: 好友数量/注册天数 F3: 是否使用真实头像（真实头像为1，非真实头像为0） F1 = 0.1 F2 = 0.2 F3 = 0 &emsp;&emsp;请问该账号是真实账号还是虚假账号？&emsp;&emsp;方法是使用朴素贝叶斯分类器，计算下面这个计算式的值。 1P(F1|C)P(F2|C)P(F3|C)P(C) &emsp;&emsp;虽然上面这些值可以从统计资料得到，但是这里有一个问题：F1和F2是连续变量，不适宜按照某个特定值计算概率。&emsp;&emsp;一个技巧是将连续值变为离散值，计算区间的概率。比如将F1分解成[0, 0.05]、(0.05, 0.2)、[0.2, +∞]三个区间，然后计算每个区间的概率。在我们这个例子中，F1等于0.1，落在第二个区间，所以计算的时候，就使用第二个区间的发生概率。&emsp;&emsp;根据统计资料，可得： 123P(F1|C0) = 0.5, P(F1|C1) = 0.1 P(F2|C0) = 0.7, P(F2|C1) = 0.2 P(F3|C0) = 0.2, P(F3|C1) = 0.9 &emsp;&emsp;因此， 123456P(F1|C0) P(F2|C0) P(F3|C0) P(C0) = 0.5 x 0.7 x 0.2 x 0.89 = 0.0623 P(F1|C1) P(F2|C1) P(F3|C1) P(C1) = 0.1 x 0.2 x 0.9 x 0.11 = 0.00198 &emsp;&emsp;可以看到，虽然这个用户没有使用真实头像，但是他是真实账号的概率，比虚假账号高出30多倍，因此判断这个账号为真。三、性别分类的例子&emsp;&emsp;下面是一组人类身体特征的统计资料。 123456789性别 身高（英尺） 体重（磅） 脚掌（英寸）男 6 180 12 男 5.92 190 11 男 5.58 170 12 男 5.92 165 10 女 5 100 6 女 5.5 150 8 女 5.42 130 7 女 5.75 150 9 &emsp;&emsp;已知某人身高6英尺、体重130磅，脚掌8英寸，请问该人是男是女？&emsp;&emsp;根据朴素贝叶斯分类器，计算下面这个式子的值。 1P(身高|性别) x P(体重|性别) x P(脚掌|性别) x P(性别) &emsp;&emsp;这里的困难在于，由于身高、体重、脚掌都是连续变量，不能采用离散变量的方法计算概率。而且由于样本太少，所以也无法分成区间计算。怎么办？&emsp;&emsp;这时，可以假设男性和女性的身高、体重、脚掌都是正态分布，通过样本计算出均值和方差，也就是得到正态分布的密度函数。有了密度函数，就可以把值代入，算出某一点的密度函数的值。&emsp;&emsp;比如，男性的身高是均值5.855、方差0.035的正态分布。所以，男性的身高为6英尺的概率的相对值等于1.5789（大于1并没有关系，因为这里是密度函数的值，只用来反映各个值的相对可能性）。 &emsp;&emsp;有了这些数据以后，就可以计算性别的分类了。 1234P(身高=6|男) x P(体重=130|男) x P(脚掌=8|男) x P(男) = 6.1984 x e-9P(身高=6|女) x P(体重=130|女) x P(脚掌=8|女) x P(女) = 5.3778 x e-4 &emsp;&emsp;可以看到，女性的概率比男性要高出将近10000倍，所以判断该人为女性。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[maven多级项目使用slf4j+log4j，以及自定义配置文件路径]]></title>
      <url>%2F2017%2F03%2F23%2Fmaven%E5%A4%9A%E7%BA%A7%E9%A1%B9%E7%9B%AE%E4%BD%BF%E7%94%A8slf4j%2Blog4j%E4%BB%A5%E5%8F%8A%E8%87%AA%E5%AE%9A%E4%B9%89%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84%20-%20%E5%89%AF%E6%9C%AC%2F</url>
      <content type="text"><![CDATA[我的maven多级结构如下： sysimple |--integration |--commons |--pom.xml |--plugins |--pom.xml |--web |--pom.xml |--pom.xml 其中依赖情况是： web依赖于commons和plugins。plugins依赖于commons。integration中定义了打包的方法与资源文件。 首先在sysimple/pom.xml中管理slf4j的版本： 在&lt;dependencyManagement&gt;&lt;/dependencyManagement&gt;中间添加： &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.7&lt;/version&gt; &lt;/dependency&gt; 由于所有的模块均引用commons，因此只需要在commons中添加slf4j的依赖即可： &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;/dependency&gt; 下面即可使用slf4j，在需要使用的地方以如下方式使用： 123456public class StartWeb &#123; private static final Logger logger = LoggerFactory.getLogger(StartWeb.class); public static void main(String[] args)&#123; logger.info("this is a example"); &#125;&#125; 默认情况下，slf4j-log4j会在src/main/java中查找log4j.properties，如果需要指定配置文件的位置，需要在启动时手动加入Jvm的参数，我的例子中添加了-Dlog4j.configuration=file:../integration/conf/sysimple-log4j.properties。在使用绝对路径时是不需要使用file:的，linux端也不需要file:。在运行的时候，slf4j会根据你指定的路径去加载配置文件。配置文件的内容我给出以下例子， 读者可以另行查找配置文件的格式： log4j.rootLogger=INFO,system.out log4j.appender.system.out=org.apache.log4j.ConsoleAppender log4j.appender.system.out.layout=org.apache.log4j.PatternLayout log4j.appender.system.out.layout.ConversionPattern=SysimpleServer Logger--&gt;%5p{%F:%L}-%m%n log4j.logger.thisProject.file=INFO,thisProject.file.out log4j.appender.thisProject.file.out=org.apache.log4j.DailyRollingFileAppender log4j.appender.thisProject.file.out.File=../integration/logs/sysimple-logs.log log4j.appender.thisProject.file.out.layout=org.apache.log4j.PatternLayout]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[jvm垃圾收集算法]]></title>
      <url>%2F2017%2F03%2F15%2Fjvm%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95%2F</url>
      <content type="text"><![CDATA[来源：JVM虚拟机 标记-清除法最基础的收集算法是“标记-清除”（Mark-Sweep）算法，如同它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象，它的标记过程其实在前一节讲述对象标记判定时已经介绍过了。 之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其不足进行改进而得到的。 它的主要不足有两个：==一个是效率问题，标记和清除两个过程的效率都不高；另一个是空间问题，标记清除之后会产生大量不连续的内存碎片==，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 复制算法为了解决效率问题，一种称为“复制”（Copying）的收集算法出现了，它将可用==内存按容量划分为大小相等的两块，每次只使用其中的一块==。 ==当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉==。 这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。 只是这种算法的代价是将内存缩小为了原来的一半，未免太高了一点。 复制算法的执行过程如图3-3所示。 现在的商业虚拟机都采用这种收集算法来回收新生代，IBM公司的专门研究表明，新生代中的对象98%是“朝生夕死”的。所以并不需要按照1:1的比例来划分内存空间，==而是将内存分为一块较大的Eden空间和两块较小的Survivor空间==，每次使用Eden和其中一块Survivor[1]。当回收时，将Eden和Survivor中还存活着的对象一次性地复制到另外一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。 HotSpot虚拟机默认Eden和Survivor的大小比例是8:1，也就是每次新生代中可用内存空间为整个新生代容量的90%（80%+10%），只有10%的内存会被“浪费”。 当然，98%的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于10%的对象存活，当Survivor空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。 标记整理算法复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会变低。 更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 分代收集算法当前商业虚拟机的垃圾收集都采用“分代收集”（Generational Collection）算法，这种算法并没有什么新的思想，==只是根据对象存活周期的不同将内存划分为几块==。 一般是把Java堆分为新生代和老年代，这样就可以==根据各个年代的特点采用最适当的收集算法==。 在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。 而老年代中因为对象存活率高、 没有额外空间对它进行分配担保，就必须使用“标记—清理”或者“标记—整理”算法来进行回收。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[java对象的创建过程]]></title>
      <url>%2F2017%2F03%2F15%2Fjava%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%88%9B%E5%BB%BA%E8%BF%87%E7%A8%8B%2F</url>
      <content type="text"><![CDATA[对象的创建过程 来源：JVM虚拟机 Java是一门面向对象的编程语言，在Java程序运行过程中无时无刻都有对象被创建出来。在语言层面上，创建对象（例如克隆、反序列化）通常仅仅是一个new关键字而已，而在虚拟机中，对象（文中讨论的对象限于普通Java对象，不包括数组和Class对象等）的创建又是怎样一个过程呢？虚拟机遇到一条new指令时，==首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过==。 如果没有，那必须先执行相应的类加载过程。 ==在类加载检查通过后，接下来虚拟机将为新生对象分配内存==。==对象所需内存的大小在类加载完成后便可完全确定==（如何确定将在2.3.2节中介绍），为对象分配空间的任务等同于把一块确定大小的内存从Java堆中划分出来。++假设Java堆中内存是绝对规整的++，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间那边挪动一段与对象大小相等的距离，这种分配方式称为“++==指针碰撞==++”（Bump the Pointer）。如果Java堆中的内存并++不是规整的++，已使用的内存和空闲的内存相互交错，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式称为“==空闲列表==”（Free List）。 选择哪种分配方式由Java堆是否规整决定，而Java堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 因此，在使用Serial、 ParNew等带Compact过程的收集器时，系统采用的分配算法是指针碰撞，而使用CMS这种基于Mark-Sweep算法的收集器时，通常采用空闲列表。 除如何划分可用空间之外，还有另外一个需要考虑的问题是对象创建在虚拟机中是非常频繁的行为，即使是仅仅修改一个指针所指向的位置，在并发情况下也并不是线程安全的，可能出现正在给对象A分配内存，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存的情况。解决这个问题有两种方案，==一种是对分配内存空间的动作进行同步处理==——实际上虚拟机采用CAS配上失败重试的方式保证更新操作的原子性；==另一种是把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer,TLAB）==。哪个线程要分配内存，就在哪个线程的TLAB上分配，只有TLAB用完并分配新的TLAB时，才需要同步锁定。虚拟机是否使用TLAB，可以通过-XX：+/-UseTLAB参数来设定。 内存分配完成后，==虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），如果使用TLAB，这一工作过程也可以提前至TLAB分配时进行==。 这一步操作保证了对象的实例字段在Java代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 接下来，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、 如何才能找到类的元数据信息、 对象的哈希码、对象的GC分代年龄等信息。 这些信息存放在对象的对象头（Object Header）之中。根据虚拟机当前的运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 关于对象头的具体内容，稍后再做详细介绍。 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从Java程序的视角来看，对象创建才刚刚开始——＜init＞方法还没有执行，所有的字段都还为零。所以，一般来说（由字节码中是否跟随invokespecial指令所决定），==执行new指令之后会接着执行＜init＞方法==，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Storm和Spark streaming对比]]></title>
      <url>%2F2017%2F02%2F28%2FStorm%E5%92%8CSpark-streaming%E5%AF%B9%E6%AF%94%2F</url>
      <content type="text"><![CDATA[Spark Streaming与Storm的应用场景对于Storm来说：1、建议在那种需要纯实时，不能忍受1秒以上延迟的场景下使用，比如实时金融系统，要求纯实时进行金融交易和分析2、此外，如果对于实时计算的功能中，要求可靠的事务机制和可靠性机制，即数据的处理完全精准，一条也不能多，一条也不能少，也可以考虑使用Storm3、如果还需要针对高峰低峰时间段，动态调整实时计算程序的并行度，以最大限度利用集群资源（通常是在小型公司，集群资源紧张的情况），也可以考虑用Storm4、如果一个大数据应用系统，它就是纯粹的实时计算，不需要在中间执行SQL交互式查询、复杂的transformation算子等，那么用Storm是比较好的选择。 对于Spark Streaming来说：1、如果对上述适用于Storm的三点，一条都不满足的实时场景，即，不要求纯实时，不要求强大可靠的事务机制，不要求动态调整并行度，那么可以考虑使用Spark Streaming2、考虑使用Spark Streaming最主要的一个因素，应该是针对整个项目进行宏观的考虑，即，如果一个项目除了实时计算之外，还包括了离线批处理、交互式查询等业务功能，而且实时计算中，可能还会牵扯到高延迟批处理、交互式查询等功能，那么就应该首选Spark生态，用Spark Core开发离线批处理，用Spark SQL开发交互式查询，用Spark Streaming开发实时计算，三者可以无缝整合，给系统提供非常高的可扩展性 Spark Streaming与Storm的优劣分析事实上，Spark Streaming绝对谈不上比Storm优秀。这两个框架在实时计算领域中，都很优秀，只是擅长的细分场景并不相同。Spark Streaming仅仅在吞吐量上比Storm要优秀，而吞吐量这一点，也是历来挺Spark Streaming，贬Storm的人着重强调的。但是问题是，是不是在所有的实时计算场景下，都那么注重吞吐量？不尽然。因此，通过吞吐量说Spark Streaming强于Storm，不靠谱。事实上，Storm在实时延迟度上，比Spark Streaming就好多了，前者是纯实时，后者是准实时。而且，Storm的事务机制、健壮性 / 容错性、动态调整并行度等特性，都要比Spark Streaming更加优秀。Spark Streaming，有一点是Storm绝对比不上的，就是：它位于Spark生态技术栈中，因此Spark Streaming可以和Spark Core、Spark SQL无缝整合，也就意味着，我们可以对实时处理出来的中间数据，立即在程序中无缝进行延迟批处理、交互式查询等操作。这个特点大大增强了Spark Streaming的优势和功能。 Storm和Spark streaming对比 对比点 storm Spark Streaming \实时计算模型&emsp; \纯实时，来一条数据，处理一条数据 \准实时，对一个时间段内的数据收集起来，作为一个RDD，再处理 \实时计算延迟度&emsp; \毫秒级 \秒级 \吞吐量&emsp; \低 \高 \事务机制&emsp; \支持完善 \支持，但不够完善 \健壮性 / 容错性&emsp; \ZooKeeper，Acker，非常强 \Checkpoint，WAL，一般 \动态调整并行度&emsp; \支持 \不支持]]></content>
    </entry>

    
  
  
</search>
