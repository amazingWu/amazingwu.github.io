<!DOCTYPE html>
<html lang="zh-CN">
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

<!-- Head tag -->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <!--Description-->
  
  <meta name="description" content="目前就读于北京理工大学软件学院">
  

  <!--Author-->
  
  <meta name="author" content="AmazingWu">
  

  <!--Open Graph Title-->
  
      <meta property="og:title" content="机器学习初涉-高斯混合模型聚类"/>
  
  <!--Open Graph Description-->
  
      <meta property="og:description" content="目前就读于北京理工大学软件学院" />
  
  <!--Open Graph Site Name-->
  <meta property="og:site_name" content="AmazingWu"/>
  <!--Type page-->
  
      <meta property="og:type" content="article" />
  
  <!--Page Cover-->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <!-- 百度统计 -->
    <script>
	var _hmt = _hmt || [];
	(function() {
  	var hm = document.createElement("script");
  	hm.src = "https://hm.baidu.com/hm.js?c0451e16533956173997b85f7a8de666";
  	var s = document.getElementsByTagName("script")[0]; 
  	s.parentNode.insertBefore(hm, s);
	})();
    </script>
  <!-- Title -->
  
  <title>机器学习初涉-高斯混合模型聚类 - AmazingWu</title>


  <link rel="shortcut icon" href="/favicon.ico">
    <!--font-awesome-->
  <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css">
  <!-- Custom CSS/Sass -->
  <link rel="stylesheet" href="/css/style.css">
<script src="https://cdn1.lncld.net/static/js/av-min-1.2.1.js"></script>
  <script>AV.initialize("vKLeKzRXEBuUzhmWMw2T1uee-gzGzoHsz", "Jg5qaNmuHFK0hn5xQ4YfpBc6");</script>
</head>


<body>

  <!-- Nav -->
  <header class="site-header">
  <div class="header-inside">
    
    <div class="logo">
      <a href="/" rel="home">
        
        <img src="/images/header.png" alt="AmazingWu" height="60">
        
      </a>
    </div>
    <a class="header-name" href="/">
            <span>AmazingWu</span>
            的部落宅
        </a>
    <!-- navbar -->
    <nav class="navbar">
      <!--  nav links -->
      <div class="collapse">
        <ul class="navbar-nav">
          
          
            <li>
              <a href="/.">
                
                  <i class="fa fa-home "></i>
                
                首页
              </a>
            </li>
          
            <li>
              <a href="/archives">
                
                  <i class="fa fa-archive "></i>
                
                归档
              </a>
            </li>
          
            <li>
              <a href="/about">
                
                  <i class="fa fa-user "></i>
                
                关于我
              </a>
            </li>
          
            <li>
              <a href="/project">
                
                  <i class="fa fa-folder-open "></i>
                
                感兴趣的项目
              </a>
            </li>
          
            <li>
              <a href="http://amazingwu.ittun.com/">
                
                  <i class="fa fa-photo "></i>
                
                相册
              </a>
            </li>
          
        </ul>
      </div>
      <!-- /.navbar-collapse -->
    </nav>
    <div class="button-wrap">
      <button class="menu-toggle">Primary Menu</button>
    </div>
  </div>
</header>


  <!-- Main Content -->
  <div class="content-area">
  <div class="post">
    <!-- Post Content -->
    <div class="container">
      <article>
        <!-- Title date & tags -->
        <div class="post-header">
          <h1 class="entry-title">
            机器学习初涉-高斯混合模型聚类
            
          </h1>
         
        </div>
         <p class="a-posted-on">
          2017-05-14
          </p>
		  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
        <!-- Post Main Content -->
        <div class="entry-content">
          <blockquote>
<p>转载自： <a href="http://blog.pluskid.org/?p=39" target="_blank" rel="external">http://blog.pluskid.org/?p=39</a> 漫谈 Clustering (3): Gaussian Mixture Model</p>
</blockquote>
<p>实上，GMM 和 k-means 很像，不过 GMM 是学习出一些概率密度函数来（所以 GMM 除了用在 clustering 上之外，还经常被用于 density estimation ），简单地说，k-means 的结果是每个数据点被 assign 到其中某一个 cluster 了，而 GMM 则给出这些数据点被 assign 到每个 cluster 的概率，又称作 soft assignment 。<br><a id="more"></a><br>得出一个概率有很多好处，因为它的信息量比简单的一个结果要多，比如，我可以把这个概率转换为一个 score ，表示算法对自己得出的这个结果的把握。也许我可以对同一个任务，用多个方法得到结果，最后选取“把握”最大的那个结果；另一个很常见的方法是在诸如疾病诊断之类的场所，机器对于那些很容易分辨的情况（患病或者不患病的概率很高）可以自动区分，而对于那种很难分辨的情况，比如，49% 的概率患病，51% 的概率正常，如果仅仅简单地使用 50% 的阈值将患者诊断为“正常”的话，风险是非常大的，因此，在机器对自己的结果把握很小的情况下，会“拒绝发表评论”，而把这个任务留给有经验的医生去解决。</p>
<p>我们已经使用过k-means算法解决聚类问题。这个算法的突出优点是简单易用，计算量也不多。然而，往往过于简单也是一个缺点。假设聚类可以表示为单个点往往会过于粗糙。举一个例子，如下图所示：<br><img src="http://img.blog.csdn.net/20160804163258791" alt="image"></p>
<p>这个例子中数据位于同心圆。在这种情况下，标准的K均值由于两个圆的均值位置相同，无法把数据划分成簇（所以上面有一个绿点不知道该往哪跑，因为它没有簇）。因此，以距离模型为聚类标准的方法不一定都能成功适用。为了解决这些缺点，我们介绍一种用统计混合模型进行聚类的方法——高斯混合模型（Gaussian Mixture Model, GMM）。这种聚类方法得到的是每个样本点属于各个类的概率，而不是判定它完全属于一个类，所以有时也会被称为软聚类。</p>
<p>废话说了一堆，不过，在回到 GMM 之前，我们再稍微扯几句。我们知道，不管是机器还是人，学习的过程都可以看作是一种“归纳”的过程，在归纳的时候你需要有一些假设的前提条件，例如，当你被告知水里游的那个家伙是鱼之后，你使用“在同样的地方生活的是同一种东西”这类似的假设，归纳出“在水里游的都是鱼”这样一个结论。当然这个过程是完全“本能”的，如果不仔细去想，你也不会了解自己是怎样“认识鱼”的。另一个值得注意的地方是这样的假设并不总是完全正确的，甚至可以说总是会有这样那样的缺陷的，因此你有可能会把虾、龟、甚至是潜水员当做鱼。也许你觉得可以通过修改前提假设来解决这个问题，例如，基于“生活在同样的地方并且穿着同样衣服的是同一种东西”这个假设，你得出结论：在水里有并且身上长有鳞片的是鱼。可是这样还是有问题，因为有些没有长鳞片的鱼现在又被你排除在外了。</p>
<p>在这个问题上，机器学习面临着和人一样的问题，在机器学习中，一个学习算法也会有一个前提假设，这里被称作“归纳偏执 (bias)”（bias 这个英文词在机器学习和统计里还有其他许多的意思）。例如线性回归，目的是要找一个函数尽可能好地拟合给定的数据点，它的归纳偏执就是“满足要求的函数必须是线性函数”。一个没有归纳偏执的学习算法从某种意义上来说毫无用处，就像一个完全没有归纳能力的人一样，在第一次看到鱼的时候有人告诉他那是鱼，下次看到另一条鱼了，他并不知道那也是鱼，因为两条鱼总有一些地方不一样的，或者就算是同一条鱼，在河里不同的地方看到，或者只是看到的时间不一样，也会被他认为是不同的，因为他无法归纳，无法提取主要矛盾、忽略次要因素，只好要求所有的条件都完全一样──然而哲学家已经告诉过我们了：世界上不会有任何样东西是完全一样的，所以这个人即使是有无比强悍的记忆力，也绝学不到任何一点知识。</p>
<p>这个问题在机器学习中称作“过拟合 (Overfitting)”，例如前面的回归的问题，如果去掉“线性函数”这个归纳偏执，因为对于 N 个点，我们总是可以构造一个 N-1 次多项式函数，让它完美地穿过所有的这 N 个点，或者如果我用任何大于 N-1 次的多项式函数的话，我甚至可以构造出无穷多个满足条件的函数出来。如果假定特定领域里的问题所给定的数据个数总是有个上限的话，我可以取一个足够大的 N ，从而得到一个（或者无穷多个）“超级函数”，能够 fit 这个领域内所有的问题。然而这个（或者这无穷多个）“超级函数”有用吗？只要我们注意到学习的目的（通常）不是解释现有的事物，而是从中归纳出知识，并能应用到新的事物上，结果就显而易见了。</p>
<p>没有归纳偏执或者归纳偏执太宽泛会导致 Overfitting ，然而另一个极端──限制过大的归纳偏执也是有问题的：如果数据本身并不是线性的，强行用线性函数去做回归通常并不能得到好结果。难点正在于在这之间寻找一个平衡点。不过人在这里相对于（现在的）机器来说有一个很大的优势：人通常不会孤立地用某一个独立的系统和模型去处理问题，一个人每天都会从各个来源获取大量的信息，并且通过各种手段进行整合处理，归纳所得的所有知识最终得以统一地存储起来，并能有机地组合起来去解决特定的问题。这里的“有机”这个词很有意思，搞理论的人总能提出各种各样的模型，并且这些模型都有严格的理论基础保证能达到期望的目的，然而绝大多数模型都会有那么一些“参数”（例如 K-means 中的 k ），通常没有理论来说明参数取哪个值更好，而模型实际的效果却通常和参数是否取到最优值有很大的关系，我觉得，在这里“有机”不妨看作是所有模型的参数已经自动地取到了最优值。另外，虽然进展不大，但是人们也一直都期望在计算机领域也建立起一个统一的知识系统（例如语意网就是这样一个尝试）。  </p>
<p>废话终于说完了，回到 GMM 。按照我们前面的讨论，作为一个流行的算法，GMM 肯定有它自己的一个相当体面的归纳偏执了。其实它的假设非常简单，顾名思义，Gaussian Mixture Model ，就是假设数据服从 Mixture Gaussian Distribution ，换句话说，数据可以看作是从数个 Gaussian Distribution 中生成出来的。实际上，我们在 K-means 和 K-medoids 两篇文章中用到的那个例子就是由三个 Gaussian 分布从随机选取出来的。实际上，从中心极限定理可以看出，Gaussian 分布（也叫做正态 (Normal) 分布）这个假设其实是比较合理的，除此之外，Gaussian 分布在计算上也有一些很好的性质，所以，虽然我们可以用不同的分布来随意地构造 XX Mixture Model ，但是还是 GMM 最为流行。另外，Mixture Model 本身其实也是可以变得任意复杂的，通过增加 Model 的个数，我们可以任意地逼近任何连续的概率密分布。</p>
<p>每个 GMM 由 K 个 Gaussian 分布组成，每个 Gaussian 称为一个“Component”，这些 Component 线性加成在一起就组成了 GMM 的概率密度函数：<br><img src="http://blog.pluskid.org/latexrender/pictures/8d0bffdb2f15ae249a74167aa0007bff.png" alt="image"><br>根据上面的式子，如果我们要从 GMM 的分布中随机地取一个点的话，实际上可以分为两步：首先随机地在这 K 个 Component 之中选一个，每个 Component 被选中的概率实际上就是它的系数 πk ,选中了 Component 之后，再单独地考虑从这个 Component 的分布中选取一个点就可以了──这里已经回到了普通的 Gaussian 分布，转化为了已知的问题。</p>
<p>那么如何用 GMM 来做 clustering 呢？其实很简单，现在我们有了数据，假定它们是由 GMM 生成出来的，那么我们只要根据数据推出 GMM 的概率分布来就可以了，然后 GMM 的 K 个 Component 实际上就对应了 K 个 cluster 了。根据数据来推算概率密度通常被称作 density estimation ，特别地，当我们在已知（或假定）了概率密度函数的形式，而要估计其中的参数的过程被称作“参数估计”。</p>
<p>现在假设我们有 N 个数据点，并假设它们服从某个分布（记作 p(x) ），现在要确定里面的一些参数的值，例如，在 GMM 中，我们就需要确定 \pi_k、\mu_k 和 \Sigma_k 这些参数。 我们的想法是，找到这样一组参数，它所确定的概率分布生成这些给定的数据点的概率最大，而这个概率实际上就等于<img src="http://blog.pluskid.org/latexrender/pictures/e88a54b57dab90eed442991ca595498d.png" alt="image">我们把这个乘积称作似然函数 (Likelihood Function)。通常单个点的概率都很小，许多很小的数字相乘起来在计算机里很容易造成浮点数下溢，因此我们通常会对其取对数，把乘积变成加和<img src="http://blog.pluskid.org/latexrender/pictures/70c6bda4c4ae7d5a1703450cd5358398.png" alt="image">得到 log-likelihood function 。接下来我们只要将这个函数最大化（通常的做法是求导并令导数等于零，然后解方程），亦即找到这样一组参数值，它让似然函数取得最大值，我们就认为这是最合适的参数，这样就完成了参数估计的过程。</p>
<p>下面让我们来看一看 GMM 的 log-likelihood function：<br><img src="http://blog.pluskid.org/latexrender/pictures/9921aae9b012b629ab6e96a945de39be.png" alt="image"></p>
<p>由于在对数函数里面又有加和，我们没法直接用求导解方程的办法直接求得最大值。为了解决这个问题，我们采取之前从 GMM 中随机选点的办法：分成两步，实际上也就类似于 K-means 的两步。</p>
<ol>
<li>估计数据由每个 Component 生成的概率（并不是每个 Component 被选中的概率）：对于每个数据 Xi 来说，它由第 k 个 Component 生成的概率为<img src="http://blog.pluskid.org/latexrender/pictures/108856f6e7c31412af2cdbbed5b8bf60.png" alt="image">由于式子里的 μk 和 Σk 也是需要我们估计的值，我们采用迭代法，在计算 γ(i, k) 的时候我们假定  μk 和 Σk 均已知，我们将取上一次迭代所得的值（或者初始值）。</li>
<li>估计每个 Component 的参数：现在我们假设上一步中得到的 γ(i, k) 就是正确的“数据  Xi 由 Component k 生成的概率”，亦可以当做该 Component 在生成这个数据上所做的贡献，或者说，我们可以看作Xi 这个值其中有 γ(i, k)Xi 这部分是由 Component k 所生成的。集中考虑所有的数据点，现在实际上可以看作 Component 生成了 γ(1, k)X1, … , γ(n, k)Xn 这些点。由于每个 Component 都是一个标准的 Gaussian 分布，可以很容易分布求出最大似然所对应的参数值：<br><img src="http://blog.pluskid.org/latexrender/pictures/96b3a1fb485a96d3d3e21594d2bbcd69.png" alt="image"><br>其中<img src="http://blog.pluskid.org/latexrender/pictures/c5fe9d4e6d474cc30475519bcdb6e38c.png" alt="image"> ，并且 πk 也顺理成章地可以估计为 Nk/N 。</li>
<li>重复迭代前面两步，直到似然函数的值收敛为止。</li>
</ol>

        </div>
      </article>
    </div>
    <!-- Pre or Next -->
    
	<div class="container" >
		
           <ul class="pager">
    	     
      	     <li class="previous">
              <a href="/2017/05/13/机器学习初涉-层次聚类/" rel="prev">下一篇：机器学习初涉--层次聚类</a>
             </li>
           
           
          </ul>
       </div>
　　　　<!-- Comments -->
    <div class="container">
      <div id="hot-news-wrap"></div>
<script>var yunModuleEnv = true;</script>
<script src="https://img1.cache.netease.com/f2e/tie/yun/sdk/loader.js"></script>
<script>
  var yunTieProductKey = "4012445f10d64eeda48ae75a222ef4f4";  
  var yunHotNewsWrap = "hot-news-wrap";   //放置的DOM节点ID 或 样式类
  Tie.loader("aHR0cHM6Ly9hcGkuZ2VudGllLjE2My5jb20vZXh0ZW5kL2hvdF9uZXdzX3NjcmlwdC5odG1s", true);
</script>

<section id="comment">
  <!-- <h1 class="title">Comments</h1> -->

  
		<div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div>
		<script src="https://img1.cache.netease.com/f2e/tie/yun/sdk/loader.js"></script>
		<script>
		var cloudTieConfig = {
		  url: document.location.href, 
		  sourceId: "",
		  productKey: "4012445f10d64eeda48ae75a222ef4f4",
		  target: "cloud-tie-wrapper"
		};
		var yunManualLoad = true;
		Tie.loader("aHR0cHM6Ly9hcGkuZ2VudGllLjE2My5jb20vcGMvbGl2ZXNjcmlwdC5odG1s", true);
		</script>
	
</section>


    </div>
   
　　　　
  </div>
</div>


  <!-- Footer -->
  <!-- Footer -->
<footer class="site-info">
  <p>
    <span>AmazingWu &copy; 2017</span>
    
      <span class="split">|</span>
      <span>AmazingWu的技术部落</span>
    
  </p>
</footer>


  <!-- After footer scripts -->
  <!-- scripts -->
<script src="/js/app.js"></script>



</body>

</html>
